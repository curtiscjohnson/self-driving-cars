{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhaPOG6xt0yn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Rim8iocC1Vva"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from simulation import Simulator\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from itertools import chain\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset that wraps memory for a dataloader\n",
        "class RLDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "    super().__init__()\n",
        "    self.data = []\n",
        "    for d in data:\n",
        "      self.data.append(d)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.data[index]\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "\n",
        "# Policy Network\n",
        "class PolicyNetwork(nn.Module):\n",
        "  def __init__(self, action_size=7, in_channels=3, cnn_outchannels=1, hidden_size=50):\n",
        "    super().__init__()\n",
        "\n",
        "    self.cnn = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels=in_channels, out_channels=cnn_outchannels*16, kernel_size=3, stride=1, padding=1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Conv2d(in_channels=cnn_outchannels*16, out_channels=cnn_outchannels*32, kernel_size=3, stride=1, padding=1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Conv2d(in_channels=cnn_outchannels*32, out_channels=cnn_outchannels, kernel_size=3, stride=1, padding=1),\n",
        "                            nn.ReLU()\n",
        "                            )\n",
        "\n",
        "    self.controller = nn.Sequential(\n",
        "                            nn.Linear(128*72, hidden_size),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Linear(hidden_size, action_size),\n",
        "                            nn.Softmax(dim=1)\n",
        "                            )\n",
        "  \n",
        "  def forward(self, img_batch):\n",
        "    \"\"\"Get policy from state\n",
        "\n",
        "      Args:\n",
        "          state (tensor): current state, size (batch x height, width, channels)\n",
        "\n",
        "      Returns:\n",
        "          action_dist (tensor): probability distribution over actions (batch x action_size)\n",
        "    \"\"\"\n",
        "    cnn_out = self.cnn(img_batch) #shape: N, Cout, Hout, Wout\n",
        "\n",
        "    actions = self.controller(cnn_out.flatten(start_dim=1, end_dim=-1))\n",
        "    return actions\n",
        "  \n",
        "\n",
        "# Value Network\n",
        "class ValueNetwork(nn.Module):\n",
        "  def __init__(self, action_size=7, in_channels=3, cnn_outchannels=1, hidden_size=50):\n",
        "    super().__init__()\n",
        "    hidden_size = 8\n",
        "\n",
        "    self.cnn = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels=in_channels, out_channels=cnn_outchannels*16, kernel_size=3, stride=1, padding=1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Conv2d(in_channels=cnn_outchannels*16, out_channels=cnn_outchannels*32, kernel_size=3, stride=1, padding=1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Conv2d(in_channels=cnn_outchannels*32, out_channels=cnn_outchannels, kernel_size=3, stride=1, padding=1),\n",
        "                            nn.ReLU()\n",
        "                            )\n",
        "\n",
        "    self.valuator = nn.Sequential(\n",
        "                            nn.Linear(128*72, hidden_size),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Linear(hidden_size, 1),\n",
        "                            )\n",
        "    \n",
        "  def forward(self, img_batch):\n",
        "    \"\"\"Estimate value given state\n",
        "\n",
        "      Args:\n",
        "          state (tensor): current state, size (batch x state_size)\n",
        "\n",
        "      Returns:\n",
        "          value (tensor): estimated value, size (batch)\n",
        "    \"\"\"\n",
        "    cnn_out = self.cnn(img_batch) #shape: N, Cout, Hout, Wout\n",
        "    values = self.valuator(cnn_out)\n",
        "    return values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RLTraining:\n",
        "    def __init__(self, policy_network, value_network, lr=1e-5, testReward=0, testScore=0, networkOutputSize=1, resolution_=(128, 72), actions = [-30,-20,-10,-3,0,3,10,20,30]):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        seed = 1\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "# state_size = \n",
        "        self.lr = lr\n",
        "        self.testReward, testScore = testReward, testScore\n",
        "        self.networkOutputSize = networkOutputSize #This will depend on how many output parameters your network returns\n",
        "\n",
        "        self.policy_network = policy_network.to(self.device)\n",
        "        self.value_network = value_network.to(self.device)\n",
        "        self.optim = torch.optim.Adam(chain(self.policy_network.parameters(), self.value_network.parameters()),\n",
        "                                      lr=self.lr)\n",
        "\n",
        "        self.mapParameters = {\n",
        "            \"loops\": 1,\n",
        "            \"size\": (6, 6),\n",
        "            \"expansions\": 5,\n",
        "            \"complications\": 4\n",
        "        }\n",
        "\n",
        "        self.carParameters = {\n",
        "        \"wheelbase\": random.uniform(5.5, 7.5), # inches, influences how quickly the steering will turn the car.  Larger = slower\n",
        "        \"maxSteering\": 30.0, # degrees, extreme (+ and -) values of steering\n",
        "        \"steeringOffset\": random.uniform(-.5, .5), # degrees, since the car is rarely perfectly aligned\n",
        "        \"minVelocity\": 0.0, # pixels/second, slower than this doesn't move at all.\n",
        "        \"maxVelocity\": 480.0, # pixels/second, 8 pixels/inch, so if the car can move 5 fps that gives us 480 pixels/s top speed\n",
        "        }\n",
        "\n",
        "        resolution = resolution_\n",
        "\n",
        "        cameraSettings = {\n",
        "            \"resolution\": (resolution[0], resolution[1]),\n",
        "            \"fov\": {\"diagonal\": random.uniform(74, 80)}, # realsense diagonal fov is 77 degrees IIRC\n",
        "            \"angle\": {\"roll\": random.uniform(-5, 5), \"pitch\": random.uniform(10, 20), \"yaw\": random.uniform(-5, 5)}, # don't go too crazy with these, my code should be good up to like... 45 degrees probably? But the math gets unstable\n",
        "            \"height\": random.uniform(58, 74) # 8 pixels/inch - represents how high up the camera is relative to the road\n",
        "        }\n",
        "\n",
        "        self.env = Simulator(cameraSettings)\n",
        "\n",
        "        self.testRealParameters = {\n",
        "            \"wheelbase\": 8,  # inches, influences how quickly the steering will turn the car.  Larger = slower\n",
        "            \"maxSteering\": 30.0,  # degrees, extreme (+ and -) values of steering\n",
        "            \"steeringOffset\": 0.0,  # degrees, since the car is rarely perfectly aligned\n",
        "            \"minVelocity\": 0.0,  # pixels/second, slower than this doesn't move at all.\n",
        "            \"maxVelocity\": 480.0,\n",
        "        }\n",
        "        testCameraSettings = {\n",
        "            \"resolution\": (resolution[0], resolution[1]),\n",
        "            \"fov\": {\"diagonal\": 94},  # realsense diagonal fov is 94 degrees IIRC\n",
        "            \"angle\": {\"roll\": 0, \"pitch\": 0, \"yaw\": 0},\n",
        "            \"height\": 66  # 8 pixels/inch - represents how high up the camera is relative to the road\n",
        "        }\n",
        "        self.testEnv = Simulator(testCameraSettings)\n",
        "        self.actions = actions\n",
        "\n",
        "        # here = Path(__file__).resolve()\n",
        "        # saveDir = str(datetime.now())\n",
        "        # saveDir = saveDir.replace(\" \", \"__\")\n",
        "        # saveDir = saveDir.replace(\"-\", \"_\")\n",
        "        # saveDir = saveDir.replace(\":\", \"_\")\n",
        "        # saveDir = saveDir.replace(\".\", \"_\")\n",
        "        # self.savePath = here / \"RL_results\" / saveDir\n",
        "        # self.savePath.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        self.savePath = \"./rl_models/\"\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_return(self, memory, rollout):\n",
        "        \"\"\"Return memory with calculated return in experience tuple\n",
        "\n",
        "          Args:\n",
        "              memory (list): (state, action, action_dist, return) tuples\n",
        "              rollout (list): (state, action, action_dist, reward) tuples from last rollout\n",
        "\n",
        "          Returns:\n",
        "              list: memory updated with (state, action, action_dist, return) tuples from rollout\n",
        "        \"\"\"\n",
        "        firstTime = True\n",
        "        previous = None\n",
        "        newMem = []\n",
        "        newMem1 = []\n",
        "        for newState, newAngle, newSpeed, newAction_dist, reward in reversed(rollout):\n",
        "\n",
        "            if firstTime:\n",
        "                new_return = reward\n",
        "                firstTime = False\n",
        "            else:\n",
        "                new_return = reward + previous * self.gamma\n",
        "            previous = new_return\n",
        "            newMem.append([newState, newAngle, newSpeed, newAction_dist, new_return])\n",
        "\n",
        "        for x in reversed(newMem):\n",
        "            newMem1.append(x)\n",
        "        memory.append(newMem1)\n",
        "        return memory\n",
        "\n",
        "\n",
        "    def get_action_ppo(self, state):\n",
        "        \"\"\"Sample action from the distribution obtained from the policy network\n",
        "\n",
        "          Args:\n",
        "              state (np-array): current state, size (state_size)\n",
        "\n",
        "          Returns:\n",
        "              int: angle sampled from output distribution of policy network\n",
        "              int: speed sampled from output distribution of policy network\n",
        "              array: output distribution of policy network\n",
        "        \"\"\"\n",
        "        # run the state through the network\n",
        "        state_tensor = torch.Tensor(np.array([state])).to(self.device) #state.shape = width, height, channel\n",
        "        action_distribution = self.policy_network(state_tensor).to(self.device)\n",
        "        angle = torch.multinomial(action_distribution, num_samples=1).item()  # TODO determine action based on network output(s). You may want to to set speed to a constant value\n",
        "        speed = 1.0\n",
        "        return angle, speed, action_distribution.detach()\n",
        "\n",
        "    def preprocessImage(self, image):\n",
        "        \"\"\"\n",
        "        Do any preprocessing of the image returned by the simulation here\n",
        "        \"\"\"\n",
        "        # TODO preprocess if desired\n",
        "        \n",
        "\n",
        "        return np.moveaxis(image, 2, 0)\n",
        "\n",
        "    def learn_ppo(self, memory_dataloader):\n",
        "        \"\"\"Implement PPO policy and value network updates. Iterate over your entire\n",
        "           memory the number of times indicated by policy_epochs.\n",
        "\n",
        "          Args:\n",
        "              memory_dataloader (DataLoader): dataloader with (state, action, action_dist, return, discounted_sum_rew) tensors\n",
        "        \"\"\"\n",
        "        self.policy_network.reset()\n",
        "        for epoch in range(0, self.policy_epochs):\n",
        "            for set in memory_dataloader:\n",
        "                self.optim.zero_grad()\n",
        "                loss = None\n",
        "                for state, action, action_dist, theReturn in set:\n",
        "\n",
        "                    # first set ups\n",
        "                    state = state.float().to(self.device)\n",
        "                    theReturn = theReturn.float().to(self.device)\n",
        "                    action = action.to(self.device)\n",
        "                    action_dist = action_dist.to(self.device)\n",
        "                    action_dist = action_dist.detach()\n",
        "\n",
        "                    # get the value loss\n",
        "                    # value.reset()\n",
        "                    test = self.value_network(state).squeeze()\n",
        "                    # value.deleteHidden()\n",
        "                    gc.collect()\n",
        "                    torch.cuda.empty_cache()\n",
        "                    squeezed = torch.argmax(test, dim=0)\n",
        "                    value_loss = nn.functional.mse_loss(squeezed, theReturn)\n",
        "\n",
        "                    # advantage for the policy loss\n",
        "                    advantage = theReturn - squeezed  # actual - expected\n",
        "                    advantage = advantage.detach()\n",
        "\n",
        "                    # policy loss\n",
        "                    encoded = nn.functional.one_hot(action,\n",
        "                                                    num_classes=self.networkOutputSize).bool().squeeze()\n",
        "                    policy_ratio = self.policy_network(state)[encoded] / action_dist.squeeze()[encoded]\n",
        "\n",
        "                    # prevent overfitting\n",
        "                    clip_policy_ratio = torch.clamp(policy_ratio, 1 - self.epsilon, 1 + self.epsilon)\n",
        "                    policy_loss = -1 * torch.mean(\n",
        "                        torch.minimum(policy_ratio * advantage, clip_policy_ratio * advantage))\n",
        "\n",
        "                    # combine the loss\n",
        "                    if loss is None:\n",
        "                        loss = value_loss + policy_loss\n",
        "                    else:\n",
        "                        loss += value_loss + policy_loss\n",
        "\n",
        "                    # normal dep learning things\n",
        "                loss.backward()\n",
        "                self.optim.step()\n",
        "                self.policy_network.reset()\n",
        "                self.value_network.reset()\n",
        "\n",
        "    def evaluateOnReal(self):\n",
        "        \"\"\"\n",
        "        This function tests your model on the simulated version of the course in the class. It evaluates and\n",
        "        \"\"\"\n",
        "\n",
        "        cum_rewards = []\n",
        "        score = 0\n",
        "        self.policy_network.eval()\n",
        "        with torch.no_grad():\n",
        "            for eval in range(self.iterTestEvaluations):\n",
        "                random.seed(eval)\n",
        "                state = self.testEnv.start(mapSeed=\"real\", carParameters=self.testRealParameters)\n",
        "                self.policy_network.reset()\n",
        "                state = self.preprocessImage(state)\n",
        "                done = False\n",
        "                rewards = 0\n",
        "                while not done and rewards < self.MAX_REWARD:\n",
        "                    angle, speed, _ = self.get_action_ppo(state)\n",
        "                    state = self.testEnv.step(angle, speed,display=False) #TODO if you would like to display course and camera view set display to True\n",
        "                    reward,done = self.getReward(self.testEnv)\n",
        "                    state = self.preprocessImage(state)\n",
        "                    rewards += reward\n",
        "                    score += 1\n",
        "                cum_rewards.append(rewards)\n",
        "        self.policy_network.train()\n",
        "        return np.mean(cum_rewards), score / self.iterTestEvaluations\n",
        "\n",
        "    def train(self, epochs=1000, saveEvery_epochs=100, iterTrainSamples=5, iterTestEvaluations=20, gamma=0.9, epsilon=0.2, policy_epochs=5, MAX_REWARD=3000):\n",
        "        self.epochs = epochs\n",
        "        self.saveEvery_epochs = saveEvery_epochs\n",
        "        self.iterTrainSamples = iterTrainSamples\n",
        "        self.iterTestEvaluations = iterTestEvaluations\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.policy_epochs = policy_epochs\n",
        "        self.MAX_REWARD = MAX_REWARD\n",
        "\n",
        "        self.testRewards = []\n",
        "        self.testScores = []\n",
        "        self.trainScores = []\n",
        "        self.trainRewards = []\n",
        "        self.trainRewardsMean = []\n",
        "        testReward = 0\n",
        "        testScore = 0\n",
        "        # Start main loop\n",
        "        results_ppo = []\n",
        "        results_ppo_mean = []\n",
        "        loop = tqdm(total=self.epochs, position=0, leave=False)\n",
        "\n",
        "        # create directory to save in\n",
        "\n",
        "\n",
        "        constantSpeed = 100\n",
        "        for epoch in range(self.epochs):\n",
        "\n",
        "            memory = []  # Reset memory every epoch\n",
        "            rewards = []  # Calculate average episodic reward per epoch\n",
        "            scores = []\n",
        "            # Begin experience loop\n",
        "            for episode in range(self.iterTrainSamples):\n",
        "                # Reset environment\n",
        "                state = self.env.start(mapSeed=(episode + 1) * (epoch + 1), mapParameters=self.mapParameters,\n",
        "                                       carParameters=self.carParameters)\n",
        "                score = 0\n",
        "                state = self.preprocessImage(state)\n",
        "                done = False\n",
        "                rollout = []\n",
        "                cum_reward = 0  # Track cumulative reward\n",
        "\n",
        "                # self.policy_network.reset()\n",
        "\n",
        "                # Begin episode\n",
        "                while not done and cum_reward < self.MAX_REWARD:\n",
        "                    # Get action\n",
        "                    angleIdx, speed, action_dist = self.get_action_ppo(state)\n",
        "                    angle = self.actions[angleIdx]\n",
        "                    # Take step\n",
        "                    next_state = self.env.step(angle, speed,display=False) #TODO if you would like to display course and camera view set display to True\n",
        "                    reward, done = self.getReward(self.env)\n",
        "                    # Store step\n",
        "                    rollout.append((state, angle, speed, action_dist, reward))\n",
        "                    score += 1\n",
        "                    cum_reward += reward\n",
        "                    state = self.preprocessImage(next_state)\n",
        "\n",
        "                # Calculate returns and add episode to memory\n",
        "                memory = self.calculate_return(memory, rollout)\n",
        "                rewards.append(cum_reward)\n",
        "                scores.append(score)\n",
        "            # Train\n",
        "            dataset = RLDataset(memory)\n",
        "            loader = DataLoader(dataset, batch_size=1, shuffle=True, drop_last=True)\n",
        "            self.policy_network.reset()\n",
        "\n",
        "            # Print results\n",
        "            self.trainRewards.extend(rewards)  # Store rewards for this epoch\n",
        "            self.trainScores.append(np.mean(scores))\n",
        "            self.trainRewardsMean.append(np.mean(rewards))\n",
        "            loop.update(1)\n",
        "            loop.set_description(\n",
        "                \"Epochs: {} Reward: {} TrainScore: {} TestReward: {} TestScore: {}\".format(epoch, np.mean(rewards),\n",
        "                                                                                           np.mean(scores), testReward,\n",
        "                                                                                           testScore))\n",
        "\n",
        "            if not (epoch % self.saveEvery_epochs):\n",
        "                # save network parameters and plots of training and testing results every desired amount of epochs.\n",
        "                # TODO you may want to also save if the average reward or score was extra high.\n",
        "                testReward, testScore = self.evaluateOnReal()\n",
        "                self.testRewards.append(testReward)\n",
        "                self.testScores.append(testScore)\n",
        "                self.saveResults(epoch)\n",
        "            else:\n",
        "                self.testRewards.append(testReward)\n",
        "                self.testScores.append(testScore)\n",
        "            self.learn_ppo(loader)\n",
        "        return results_ppo\n",
        "        \n",
        "    def saveResults(self,epoch):\n",
        "        policyPath = self.savePath / (\"policy_epoch_\" + str(epoch)  + \".pt\")\n",
        "        valuePath = self.savePath / (\"value_epoch_\" + str(epoch)  + \".pt\")\n",
        "        plotPath = self.savePath / (\"plot_epoch_\" + str(epoch) + \".png\")\n",
        "        plotMeanPath = self.savePath / (\"plot_epoch_mean_\" + str(epoch) + \".png\")\n",
        "        plotScorePath = self.savePath / (\"plot_epoch_score_\" + str(epoch) + \".png\")\n",
        "\n",
        "        torch.save(self.policy_network.state_dict(), str(policyPath))\n",
        "        torch.save(self.value_network.state_dict(), str(valuePath))\n",
        "        plt.figure().clear()\n",
        "        plt.close()\n",
        "        plt.cla()\n",
        "        plt.clf()\n",
        "        plt.plot(self.trainRewards)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "        plt.savefig(plotPath)\n",
        "        plt.figure().clear()\n",
        "        plt.close()\n",
        "        plt.cla()\n",
        "        plt.clf()\n",
        "        plt.plot(self.trainRewardsMean, label=\"Train\")\n",
        "        plt.plot(self.testRewards, label=\"Test\")\n",
        "        plt.legend()\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "        plt.savefig(plotMeanPath)\n",
        "        plt.close()\n",
        "        plt.cla()\n",
        "        plt.clf()\n",
        "        plt.plot(self.trainScores, label=\"Train\")\n",
        "        plt.plot(self.testScores, label=\"Test\")\n",
        "        plt.legend()\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.savefig(plotScorePath)\n",
        "\n",
        "    def getReward(self, env):\n",
        "        # return negative reward if crashed positive reward if doing\n",
        "        distToCenter, bearingOffset = env.getStats()\n",
        "\n",
        "        bearingThreshold = .3\n",
        "        offTheRoadThresh = 40\n",
        "        crashed = False\n",
        "        if distToCenter < offTheRoadThresh/2:\n",
        "\n",
        "            reward = np.clip(1 - (bearingOffset / bearingThreshold), -1, 1)\n",
        "        else:\n",
        "            reward = -1\n",
        "\n",
        "        if distToCenter > offTheRoadThresh:\n",
        "            crashed=True\n",
        "        return reward, crashed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for +=: 'int' and 'NoneType'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[69], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m value_network \u001b[38;5;241m=\u001b[39m ValueNetwork(action_size\u001b[38;5;241m=\u001b[39maction_size)\n\u001b[1;32m      7\u001b[0m training \u001b[38;5;241m=\u001b[39m RLTraining(policy_network, value_network, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, testReward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, testScore\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, networkOutputSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, resolution_\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m72\u001b[39m), actions \u001b[38;5;241m=\u001b[39m actions)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaveEvery_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterTrainSamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterTestEvaluations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_REWARD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[68], line 268\u001b[0m, in \u001b[0;36mRLTraining.train\u001b[0;34m(self, epochs, saveEvery_epochs, iterTrainSamples, iterTestEvaluations, gamma, epsilon, policy_epochs, MAX_REWARD)\u001b[0m\n\u001b[1;32m    266\u001b[0m     rollout\u001b[38;5;241m.\u001b[39mappend((state, angle, speed, action_dist, reward))\n\u001b[1;32m    267\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 268\u001b[0m     cum_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    269\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessImage(next_state)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Calculate returns and add episode to memory\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'NoneType'"
          ]
        }
      ],
      "source": [
        "actions = [-30,-20,-10,-3,0,3,10,20,30]\n",
        "action_size = len(actions)\n",
        "\n",
        "policy_network = PolicyNetwork(action_size=action_size)\n",
        "value_network = ValueNetwork(action_size=action_size)\n",
        "\n",
        "training = RLTraining(policy_network, value_network, lr=1e-5, testReward=0, testScore=0, networkOutputSize=2, resolution_=(128, 72), actions = actions)\n",
        "\n",
        "training.train(epochs=1000, saveEvery_epochs=100, iterTrainSamples=5, iterTestEvaluations=20, gamma=0.9, epsilon=0.2, policy_epochs=5, MAX_REWARD=3000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
