{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Type, Union\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch import nn\n",
    "\n",
    "from stable_baselines3.common.preprocessing import get_flattened_obs_dim, is_image_space\n",
    "from stable_baselines3.common.type_aliases import TensorDict\n",
    "from stable_baselines3.common.utils import get_device\n",
    "\n",
    "\n",
    "class BaseFeaturesExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class that represents a features extractor.\n",
    "\n",
    "    :param observation_space:\n",
    "    :param features_dim: Number of features extracted.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        assert features_dim > 0\n",
    "        self._observation_space = observation_space\n",
    "        self._features_dim = features_dim\n",
    "\n",
    "    @property\n",
    "    def features_dim(self) -> int:\n",
    "        return self._features_dim\n",
    "\n",
    "\n",
    "class FlattenExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Feature extract that flatten the input.\n",
    "    Used as a placeholder when feature extraction is not needed.\n",
    "\n",
    "    :param observation_space:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.Space) -> None:\n",
    "        super().__init__(observation_space, get_flattened_obs_dim(observation_space))\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.flatten(observations)\n",
    "\n",
    "\n",
    "class NatureCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    CNN from DQN Nature paper:\n",
    "        Mnih, Volodymyr, et al.\n",
    "        \"Human-level control through deep reinforcement learning.\"\n",
    "        Nature 518.7540 (2015): 529-533.\n",
    "\n",
    "    :param observation_space:\n",
    "    :param features_dim: Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    :param normalized_image: Whether to assume that the image is already normalized\n",
    "        or not (this disables dtype and bounds checks): when True, it only checks that\n",
    "        the space is a Box and has 3 dimensions.\n",
    "        Otherwise, it checks that it has expected dtype (uint8) and bounds (values in [0, 255]).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Box,\n",
    "        action_space,\n",
    "        features_dim: int = 512,\n",
    "        normalized_image: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "        assert is_image_space(observation_space, check_channels=False, normalized_image=normalized_image), (\n",
    "            \"You should use NatureCNN \"\n",
    "            f\"only with images not with {observation_space}\\n\"\n",
    "            \"(you are probably using `CnnPolicy` instead of `MlpPolicy` or `MultiInputPolicy`)\\n\"\n",
    "            \"If you are using a custom environment,\\n\"\n",
    "            \"please check it using our env checker:\\n\"\n",
    "            \"https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html.\\n\"\n",
    "            \"If you are using `VecNormalize` or already normalized channel-first images \"\n",
    "            \"you should pass `normalize_images=False`: \\n\"\n",
    "            \"https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html\"\n",
    "        )\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "            print(n_flatten)\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "        self.action_output = nn.Linear(features_dim, len(action_space))\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        # print(self.cnn(observations).flatten().shape)\n",
    "        out = self.linear(self.cnn(observations).flatten())\n",
    "        return self.action_output(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (64, 64)\n",
    "cameraSettings = {\n",
    "    # \"resolution\": (1920, 1080),\n",
    "    \"resolution\": img_size,\n",
    "    \"fov\": {\"diagonal\": 77},  # realsense diagonal fov is 77 degrees IIRC\n",
    "    \"angle\": {\n",
    "        \"roll\": 0,\n",
    "        \"pitch\": 0,\n",
    "        \"yaw\": 0,\n",
    "    },  # don't go too crazy with these, my code should be good up to like... 45 degrees probably? But the math gets unstable\n",
    "    # \"angle\": {\"roll\": 13, \"pitch\": 30, \"yaw\": 30}, # don't go too crazy with these, my code should be good up to like... 45 degrees probably? But the math gets unstable\n",
    "    \"height\": 66,  # 8 pixels/inch - represents how high up the camera is relative to the road\n",
    "}\n",
    "\n",
    "mapParameters = {\"loops\": 1, \"size\": (6, 6), \"expansions\": 5, \"complications\": 4}\n",
    "\n",
    "# Can also pass car parameters for max/min speed, etc\n",
    "carParameters = {\n",
    "    \"wheelbase\": 6.5,  # inches, influences how quickly the steering will turn the car.  Larger = slower\n",
    "    \"maxSteering\": 30.0,  # degrees, extreme (+ and -) values of steering\n",
    "    \"steeringOffset\": 0.0,  # degrees, since the car is rarely perfectly aligned\n",
    "    \"minVelocity\": 0.0,  # pixels/second, slower than this doesn't move at all.\n",
    "    \"maxVelocity\": 480.0,  # pixels/second, 8 pixels/inch, so if the car can move 5 fps that gives us 480 pixels/s top speed\n",
    "}\n",
    "\n",
    "# taken from https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml\n",
    "config = {\n",
    "    \"n_timesteps\": 1e6,  # sb3 dqn runs go up to 1e7 at most\n",
    "    \"policy\": \"CnnPolicy\",\n",
    "    \"env\": \"CustomDuckieTown\",\n",
    "    \"actions\": [-30,-15, 0, 15, 30],\n",
    "    \"camera_settings\": cameraSettings,\n",
    "    \"map_parameters\": mapParameters,\n",
    "    \"car_parameters\": carParameters,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_starts\": 100000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"target_update_interval\": 1000,\n",
    "    \"train_freq\": 4,\n",
    "    \"gradient_steps\": 1,\n",
    "    \"exploration_fraction\": 0.1,\n",
    "    \"exploration_final_eps\": 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "NatureCNN(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (action_output): Linear(in_features=512, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "N_DISCRETE_ACTIONS = len(config['actions'])\n",
    "action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "\n",
    "N_CHANNELS = 3\n",
    "(HEIGHT, WIDTH) = cameraSettings[\"resolution\"]\n",
    "observation_space = spaces.Box(\n",
    "    low=0, high=1, shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8\n",
    ")\n",
    "#!had to change channels to beginning!!! it was at the end before\n",
    "\n",
    "model = NatureCNN(observation_space, config['actions'], normalized_image=True)\n",
    "\n",
    "# print(torch.load(\"./CUSTOM_SAVE.pt\").keys())\n",
    "model.load_state_dict(torch.load(\"./CUSTOM_SAVE.pt\"))\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomDuckieTownEnv import CustomDuckieTownSim\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "def make_env(display, config):\n",
    "    env = CustomDuckieTownSim(\n",
    "        config[\"camera_settings\"],\n",
    "        config[\"map_parameters\"],\n",
    "        config[\"car_parameters\"],\n",
    "        config[\"actions\"],\n",
    "        display,\n",
    "    )\n",
    "    env = Monitor(env)  # record stats such as returns\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 64])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = make_env(True, config)\n",
    "obs = env.reset()\n",
    "# print(torch.from_numpy(obs).shape)\n",
    "while True:\n",
    "    print((torch.from_numpy(obs/255).float()).shape)\n",
    "    action = model(torch.from_numpy(obs/255).float()).max(0)[1].view(1,1)\n",
    "    print(config['actions'][action])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
