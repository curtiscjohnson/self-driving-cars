{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhaPOG6xt0yn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Rim8iocC1Vva"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "import gc\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from simulation import Simulator, coordinate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulator Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reset_sim():\n",
        "    \n",
        "    # Units are pixels for resolution, degrees for fov, degrees for angle, and pixels for height.\n",
        "    cameraSettings = {\n",
        "        \"resolution\": (1920, 1080),\n",
        "        \"fov\": {\"diagonal\": random.uniform(74, 80)}, # realsense diagonal fov is 77 degrees IIRC\n",
        "        \"angle\": {\"roll\": random.uniform(-5, 5), \"pitch\": random.uniform(10, 20), \"yaw\": random.uniform(-5, 5)}, # don't go too crazy with these, my code should be good up to like... 45 degrees probably? But the math gets unstable\n",
        "        \"height\": random.uniform(58, 74) # 8 pixels/inch - represents how high up the camera is relative to the road\n",
        "    }\n",
        "\n",
        "    mapParameters = {\n",
        "    \"loops\": 1,\n",
        "    \"size\": (6, 6),\n",
        "    \"expansions\": 5,\n",
        "    \"complications\": 4\n",
        "    }\n",
        "\n",
        "    # Can also pass car parameters for max/min speed, etc\n",
        "    carParameters = {\n",
        "        \"wheelbase\": random.uniform(5.5, 7.5), # inches, influences how quickly the steering will turn the car.  Larger = slower\n",
        "        \"maxSteering\": 30.0, # degrees, extreme (+ and -) values of steering\n",
        "        \"steeringOffset\": random.uniform(-.5, .5), # degrees, since the car is rarely perfectly aligned\n",
        "        \"minVelocity\": 0.0, # pixels/second, slower than this doesn't move at all.\n",
        "        \"maxVelocity\": 480.0, # pixels/second, 8 pixels/inch, so if the car can move 5 fps that gives us 480 pixels/s top speed\n",
        "    }\n",
        "\n",
        "    sim = Simulator(cameraSettings=cameraSettings)\n",
        "    \n",
        "    # startLocation = (random.randint(0, 5), random.randint(0, 5), 0, random.randint(0, 2))\n",
        "    # random seed for consistent maps\n",
        "    # can also pass a start location if you know the code: (y tile index, x tile index, position index, direction index)\n",
        "    # - position index is from 0-(number of connections the tile has - 1), so a straight is 0 or 1, a t is 0, 1, or 2.\n",
        "    # - direction index is 0 or 1 for normal or reversed.\n",
        "    sim.start(mapSeed='real', mapParameters=mapParameters, carParameters=carParameters, startPoint=(0,4,0,0))\n",
        "    where, facing = sim.RealSense.parent.ackermann.pose()\n",
        "    initial_img = sim.RealSense.camera.getImage(where, facing)\n",
        "    return sim, initial_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From here, the API for using the simulation is as follows. Steps the entire simulation, returns image, reward from sim.getReward() and a done bool (and we can change what 'done' means. Currently its if reward is negative):\n",
        "\n",
        "```python\n",
        "frame, reward, done = sim.step(steer, speed, display=False) \n",
        "```\n",
        "\n",
        "In order to reset the simulation, you just need to reconstruct the sim object and start it, using the reset_sim() function above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV282uYJ2aSw"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi_aDdTg2btp"
      },
      "source": [
        "Deep Q-Network (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) is a Q-learning algorithm that learns values for state-action pairs.\n",
        "\n",
        "Actions are sampled according to an $\\epsilon-greedy$ policy to help with exploration of the state space. Every time an action is sampled, the agent chooses a random action with $\\epsilon$ probability. Otherwise, the agent selects the action with the highest Q-value for a state. $\\epsilon$ decays over time according to $\\epsilon \\gets \\epsilon * epsilon\\_decay$.\n",
        "\n",
        "Tuples of state, action, reward, next_state, and terminal $(s,a,r,s',d)$ are collected during training. Every $learn\\_frequency$ steps $sample\\_size$ tuples are sampled and made into 5 tensors tensors of states, actions, rewarads, next_states, and terminals.\n",
        "\n",
        "The loss for a batch of size N is given below.\n",
        "\n",
        "$Loss=\\frac{1}{N}\\sum \\bigg(Q(s,a) - (r + \\gamma \\underset{a'\\sim A}{max} \\hat{Q}(s',a')(1-d))\\bigg)^2 $\n",
        "\n",
        "Loss is calculated and used to update the Q-Network. The target network $\\hat{Q}$ begins as a copy of the Q network but is not updated by the optimizer. Every $target\\_update$ steps, the target network is updated with the parameters of the Q-Network. This process is a type of bootstrapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q-Value Network\n",
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, feature_size=20, action_size=7):\n",
        "    super().__init__()\n",
        "    \n",
        "    hidden_size = 50\n",
        "    \n",
        "    self.resnet18 = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
        "    self.resnet18.fc = nn.Linear(in_features=self.resnet18.fc.in_features, out_features=feature_size)\n",
        "\n",
        "    self.label = nn.Linear(feature_size, action_size)\n",
        "\n",
        "\n",
        "    # self.lstm = nn.LSTM(feature_size, hidden_size, num_layers=1, batch_first=True)\n",
        "    # self.label = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    # self.prev_hidden_state = torch.zeros(self.lstm.num_layers, hidden_size).cuda()\n",
        "    # self.prev_cell_state =  torch.zeros(self.lstm.num_layers, hidden_size).cuda()\n",
        "    \n",
        "  def forward(self, img_batch):\n",
        "    \"\"\"Estimate q-values given image\n",
        "\n",
        "      Args:\n",
        "          img batch (4d tensor): size (batch_size, height, width, channel)\n",
        "\n",
        "      Returns:\n",
        "          q-values (tensor): estimated q-values, size (batch x action_size)\n",
        "    \"\"\"\n",
        "    img_batch = transforms.functional.convert_image_dtype(img_batch).permute([0, 3, 1, 2])\n",
        "    features = self.resnet18(img_batch)\n",
        "\n",
        "    # output, (final_hidden_state, final_cell_state) = self.lstm(features, (self.prev_hidden_state, self.prev_cell_state))\n",
        "\n",
        "    # self.prev_hidden_state = final_hidden_state.detach()\n",
        "    # self.prev_cell_state = final_cell_state.detach()\n",
        "\n",
        "    # values = self.label(output)\n",
        "    values = self.label(features)\n",
        "    return values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_mBUvXkT2dHy"
      },
      "outputs": [],
      "source": [
        "def get_action_dqn(network, state, epsilon, epsilon_decay):\n",
        "  \"\"\"Select action according to e-greedy policy and decay epsilon\n",
        "\n",
        "    Args:\n",
        "        network (QNetwork): Q-Network\n",
        "        state (np-array): current state, size (state_size)\n",
        "        epsilon (float): probability of choosing a random action\n",
        "        epsilon_decay (float): amount by which to decay epsilon\n",
        "\n",
        "    Returns:\n",
        "        action (int): chosen action [0, action_size)\n",
        "        epsilon (float): decayed epsilon\n",
        "  \"\"\"\n",
        "  \n",
        "  if random.uniform(0., 1.) < epsilon:\n",
        "    action = random.randint(0,6) #randint 0-6 corresponding to [-30,-20,-10,0,10,20,30] degrees\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      state_tensor = torch.Tensor(state).float().cuda()\n",
        "      action = int(np.argmax(network(state_tensor.unsqueeze(0)).cpu()))\n",
        "      # del state_tensor\n",
        "  return action, epsilon*epsilon_decay\n",
        "\n",
        "\n",
        "def prepare_batch(frame_buffer, action_idx_buffer, next_frame_buffer, reward_buffer, done_buffer, batch_size):\n",
        "  \"\"\"Randomly sample batch from memory\n",
        "     Prepare cuda tensors\n",
        "\n",
        "    Args:\n",
        "        memory (list): state, action, next_state, reward, done tuples\n",
        "        batch_size (int): amount of memory to sample into a batch\n",
        "\n",
        "    Returns:\n",
        "        state (tensor): float cuda tensor of size (batch_size x state_size)\n",
        "        action (tensor): long tensor of size (batch_size)\n",
        "        next_state (tensor): float cuda tensor of size (batch_size x state_size)\n",
        "        reward (tensor): float cuda tensor of size (batch_size)\n",
        "        done (tensor): float cuda tensor of size (batch_size)\n",
        "  \"\"\"\n",
        "\n",
        "  idx_array = np.random.randint(0, len(frame_buffer), batch_size)\n",
        "\n",
        "\n",
        "  frame = np.array([frame_buffer[i] for i in idx_array])\n",
        "  action = np.array([action_idx_buffer[i] for i in idx_array])\n",
        "  next_frame = np.array([next_frame_buffer[i] for i in idx_array])\n",
        "  reward = np.array([reward_buffer[i] for i in idx_array])\n",
        "  done = np.array([done_buffer[i] for i in idx_array])\n",
        "\n",
        "  # frame = np.array(frame_buffer[idx_array])\n",
        "  # action = np.array(action_idx_buffer[idx_array])\n",
        "  # next_frame = np.array(next_frame_buffer[idx_array])\n",
        "  # reward = np.array(reward_buffer[idx_array])\n",
        "  # done = np.array(done_buffer[idx_array])\n",
        "\n",
        "  return torch.FloatTensor(frame).cuda(), torch.FloatTensor(action).cuda(),torch.FloatTensor(next_frame).cuda(), \\\n",
        "          torch.FloatTensor(reward).cuda(), torch.FloatTensor(done).cuda()\n",
        "  \n",
        "def learn_dqn(trajectory, optim, q_network, target_network, gamma, episode, target_update):\n",
        "  \"\"\"Update Q-Network according to DQN Loss function\n",
        "     Update Target Network every target_update global steps\n",
        "\n",
        "    Args:\n",
        "        batch (tuple): tuple of state, action, next_state, reward, and done tensors\n",
        "        optim (Adam): Q-Network optimizer\n",
        "        q_network (QNetwork): Q-Network\n",
        "        target_network (QNetwork): Target Q-Network\n",
        "        gamma (float): discount factor\n",
        "        episode (int): total steps taken in environment\n",
        "        target_update (int): frequency of target network update\n",
        "  \"\"\"\n",
        "  optim.zero_grad()\n",
        "  total_loss = torch.tensor([0.0], requires_grad=True).cuda()\n",
        "\n",
        "  state = trajectory[0]\n",
        "  action =trajectory[1]\n",
        "  next_state = trajectory[2]\n",
        "  reward = trajectory[3] \n",
        "  done = trajectory[4]\n",
        "\n",
        "  # Sequentially loop through the episode trajectory\n",
        "  action = torch.unsqueeze(action,dim=0).long()\n",
        "\n",
        "  Q = torch.gather(q_network(state), 1, action)\n",
        "  Q_hat = torch.max(target_network(next_state), dim=1)[0]\n",
        "  b = (reward + gamma*Q_hat*(1 - done))\n",
        "\n",
        "  Q = torch.squeeze(Q)\n",
        "  \n",
        "  total_loss = F.mse_loss(Q, b)\n",
        "\n",
        "  total_loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  if episode % target_update == 0:\n",
        "    target_network.load_state_dict(q_network.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCafVI552dgg"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sy_r9Wr2eg8",
        "outputId": "ed94a19d-7071-42cd-d62f-03014ea8158a"
      },
      "outputs": [],
      "source": [
        "def dqn_main():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Hyper parameters\n",
        "  episodes = 10000\n",
        "  target_update = 250\n",
        "  save_frequency = 250  \n",
        "  MAX_EPISODE_LENGTH = 200\n",
        "  SAVE_REWARD_THRESHOLD = 20\n",
        "  LEARN_FREQUENCY = 25\n",
        "  START_TRAINING = 200\n",
        "  BATCH_SIZE = 32\n",
        "\n",
        "  CAR_SPEED = 1.5\n",
        "\n",
        "  lr = 1e-3\n",
        "\n",
        "  gamma = 0.99\n",
        "  epsilon = 1\n",
        "  epsilon_decay = .999\n",
        "  img_size = (128, 72)\n",
        "\n",
        "  # Init networks\n",
        "  q_network = QNetwork().cuda()\n",
        "  target_network = QNetwork().cuda()\n",
        "  target_network.load_state_dict(q_network.state_dict()) #copy q_network into target_network\n",
        "\n",
        "  # Init optimizer\n",
        "  optim = torch.optim.Adam(q_network.parameters(), lr=lr)\n",
        "\n",
        "  # Begin main loop\n",
        "  results_dqn = []\n",
        "  global_step = 0\n",
        "  prev_avg_reward = 0.0\n",
        "  avg_reward = 20.0\n",
        "  loop = tqdm(total=episodes, position=0, leave=False)\n",
        "  action_space = [-30,-20,-10,0,10,20,30]\n",
        "\n",
        "  # Init episode replay buffer\n",
        "  frame_buffer = []\n",
        "  action_idx_buffer = []\n",
        "  next_frame_buffer = []\n",
        "  reward_buffer = []\n",
        "  done_buffer = []\n",
        "\n",
        "  for episode in range(1, episodes):\n",
        "\n",
        "    # Reset environment\n",
        "    sim, frame = reset_sim()\n",
        "    done = False\n",
        "    cum_reward = 0  # Track cumulative reward per episode\n",
        "\n",
        "    frame = cv2.resize(frame, img_size)\n",
        "\n",
        "    # Begin episode\n",
        "    while not done and cum_reward < MAX_EPISODE_LENGTH:  # End after 200 steps \n",
        "      # Select e-greedy action\n",
        "      action_idx, epsilon = get_action_dqn(q_network, frame, epsilon, epsilon_decay)\n",
        "\n",
        "      # Take step\n",
        "      next_frame, reward, done = sim.step(steer=action_space[action_idx], speed=CAR_SPEED, display=False)\n",
        "\n",
        "      # Store step in replay bufferimg\n",
        "      next_frame = cv2.resize(next_frame, img_size)\n",
        "\n",
        "      frame_buffer.append(next_frame)\n",
        "      action_idx_buffer.append(action_idx)\n",
        "      next_frame_buffer.append(next_frame)\n",
        "      reward_buffer.append(reward)\n",
        "      done_buffer.append(done)\n",
        "\n",
        "      cum_reward += reward\n",
        "      frame = next_frame  # Set current frame\n",
        "      global_step += 1\n",
        "\n",
        "      if (global_step > START_TRAINING) and (global_step % LEARN_FREQUENCY == 0):\n",
        "        batch = prepare_batch(frame_buffer, action_idx_buffer, next_frame_buffer, reward_buffer, done_buffer, BATCH_SIZE)  # Train the network after episode ended\n",
        "        learn_dqn(batch, optim, q_network, target_network, gamma, episode, target_update)     # Train\n",
        "        del batch\n",
        "\n",
        "    avg_reward += cum_reward\n",
        "\n",
        "    # Save the network if save_frequency steps has passed and it is better than the previous avg_reward\n",
        "    if (episode % save_frequency == 0):\n",
        "      avg_reward /= save_frequency\n",
        "      if (avg_reward > prev_avg_reward + SAVE_REWARD_THRESHOLD):\n",
        "        print(f\"{save_frequency} episodes completed. Average Reward: {avg_reward} > Previous Average: {prev_avg_reward}\")\n",
        "        torch.save(q_network, f'./rl_models/model{episode}.pt')\n",
        "      prev_avg_reward = avg_reward\n",
        "      avg_reward = 0.0\n",
        "\n",
        "    # Print results at end of episode\n",
        "    r = torch.cuda.memory_reserved(0)\n",
        "    a = torch.cuda.memory_allocated(0)\n",
        "    results_dqn.append(cum_reward)\n",
        "    loop.update(1)\n",
        "    loop.set_description('Episodes: {} Reward: {} Allocated Memory: {} Reserved Memory {}'.format(episode, cum_reward, a / 1e9, r / 1e9))\n",
        "  \n",
        "  return q_network, results_dqn, prev_avg_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episodes: 5 Reward: 50.0 Allocated Memory: 0.182882304 Reserved Memory 0.226492416:   0%|          | 5/10000 [00:01<48:38,  3.42it/s]  "
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "only integer scalar arrays can be converted to a scalar index",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model, results_dqn, avg_reward \u001b[38;5;241m=\u001b[39m \u001b[43mdqn_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(results_dqn)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
            "Cell \u001b[0;32mIn[12], line 78\u001b[0m, in \u001b[0;36mdqn_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (global_step \u001b[38;5;241m>\u001b[39m START_TRAINING) \u001b[38;5;129;01mand\u001b[39;00m (global_step \u001b[38;5;241m%\u001b[39m LEARN_FREQUENCY \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 78\u001b[0m   batch \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_idx_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_frame_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train the network after episode ended\u001b[39;00m\n\u001b[1;32m     79\u001b[0m   learn_dqn(batch, optim, q_network, target_network, gamma, episode, target_update)     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m     80\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m batch\n",
            "Cell \u001b[0;32mIn[11], line 43\u001b[0m, in \u001b[0;36mprepare_batch\u001b[0;34m(frame_buffer, action_idx_buffer, next_frame_buffer, reward_buffer, done_buffer, batch_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Randomly sample batch from memory\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m   Prepare cuda tensors\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m      done (tensor): float cuda tensor of size (batch_size)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m idx_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(frame_buffer), batch_size)\n\u001b[0;32m---> 43\u001b[0m frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(frame_buffer[idx_array])\n\u001b[1;32m     44\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action_idx_buffer[idx_array])\n\u001b[1;32m     45\u001b[0m next_frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(next_frame_buffer[idx_array])\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
          ]
        }
      ],
      "source": [
        "trained_model, results_dqn, avg_reward = dqn_main()\n",
        "\n",
        "plt.plot(results_dqn)\n",
        "plt.show()\n",
        "\n",
        "print(f'Final Average Reward: {avg_reward}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
