{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhaPOG6xt0yn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rim8iocC1Vva"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "import gc\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "import time\n",
        "from simulation import Simulator, coordinate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulator Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reset_sim():\n",
        "    # Units are pixels for resolution, degrees for fov, degrees for angle, and pixels for height.\n",
        "    img_size = (25, 25)\n",
        "    cameraSettings = {\n",
        "        # \"resolution\": (1920, 1080),\n",
        "        \"resolution\": img_size,\n",
        "        \"fov\": {\"diagonal\": 77}, # realsense diagonal fov is 77 degrees IIRC\n",
        "        \"angle\": {\"roll\": 0, \"pitch\": 0, \"yaw\": 0}, # don't go too crazy with these, my code should be good up to like... 45 degrees probably? But the math gets unstable\n",
        "        # \"angle\": {\"roll\": 13, \"pitch\": 30, \"yaw\": 30}, # don't go too crazy with these, my code should be good up to like... 45 degrees probably? But the math gets unstable\n",
        "        \"height\": 66 # 8 pixels/inch - represents how high up the camera is relative to the road\n",
        "    }\n",
        "\n",
        "    mapParameters = {\n",
        "        \"loops\": 1,\n",
        "        \"size\": (6, 6),\n",
        "        \"expansions\": 5,\n",
        "        \"complications\": 4\n",
        "    }\n",
        "\n",
        "    # Can also pass car parameters for max/min speed, etc\n",
        "    carParameters = {\n",
        "        \"wheelbase\": 6.5, # inches, influences how quickly the steering will turn the car.  Larger = slower\n",
        "        \"maxSteering\": 30.0, # degrees, extreme (+ and -) values of steering\n",
        "        \"steeringOffset\": 0.0, # degrees, since the car is rarely perfectly aligned\n",
        "        \"minVelocity\": 0.0, # pixels/second, slower than this doesn't move at all.\n",
        "        \"maxVelocity\": 480.0, # pixels/second, 8 pixels/inch, so if the car can move 5 fps that gives us 480 pixels/s top speed\n",
        "    }\n",
        "\n",
        "    sim = Simulator(cameraSettings=cameraSettings)\n",
        "\n",
        "    startLocations = np.array([[0,1], [0,2], [0,3], [0,4], [0,5], [0,6], [0,7], [1,0], [1,1], [1,4], [1,7], [2,2], [2,3], [1,4], [2,5], [2,6], [2,0], [5,1], [3,2], [2,4], [5,5], [5,6], [2,7], [3,0], [7,1], [4,2], [7,3], [5,4], [6,5], [3,7], [4,0], [5,2], [7,4], [7,5], [4,7], [5,0], [7,2], [5,7], [6,0]])\n",
        "    startLoc = random.randint(0, 38)\n",
        "\n",
        "    sim.start(mapSeed='real', mapParameters=mapParameters, carParameters=carParameters, startPoint=(int(startLocations[startLoc, 0]),int(startLocations[startLoc, 1]),0,0))\n",
        "    # sim.start(mapSeed='real', mapParameters=mapParameters, carParameters=carParameters, startPoint=(0,4,0,0))\n",
        "    \n",
        "    where, facing = sim.RealSense.parent.ackermann.pose()\n",
        "    initial_img = sim.RealSense.camera.getImage(where, facing)\n",
        "    return sim, initial_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From here, the API for using the simulation is as follows. Steps the entire simulation, returns image, reward from sim.getReward() and a done bool (and we can change what 'done' means. Currently its if reward is negative):\n",
        "\n",
        "```python\n",
        "frame, reward, done = sim.step(steer, speed, display=False) \n",
        "```\n",
        "\n",
        "In order to reset the simulation, you just need to reconstruct the sim object and start it, using the reset_sim() function above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV282uYJ2aSw"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi_aDdTg2btp"
      },
      "source": [
        "Deep Q-Network (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) is a Q-learning algorithm that learns values for state-action pairs.\n",
        "\n",
        "Actions are sampled according to an $\\epsilon-greedy$ policy to help with exploration of the state space. Every time an action is sampled, the agent chooses a random action with $\\epsilon$ probability. Otherwise, the agent selects the action with the highest Q-value for a state. $\\epsilon$ decays over time according to $\\epsilon \\gets \\epsilon * epsilon\\_decay$.\n",
        "\n",
        "Tuples of state, action, reward, next_state, and terminal $(s,a,r,s',d)$ are collected during training. Every $learn\\_frequency$ steps $sample\\_size$ tuples are sampled and made into 5 tensors tensors of states, actions, rewarads, next_states, and terminals.\n",
        "\n",
        "The loss for a batch of size N is given below.\n",
        "\n",
        "$Loss=\\frac{1}{N}\\sum \\bigg(Q(s,a) - (r + \\gamma \\underset{a'\\sim A}{max} \\hat{Q}(s',a')(1-d))\\bigg)^2 $\n",
        "\n",
        "Loss is calculated and used to update the Q-Network. The target network $\\hat{Q}$ begins as a copy of the Q network but is not updated by the optimizer. Every $target\\_update$ steps, the target network is updated with the parameters of the Q-Network. This process is a type of bootstrapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q-Value Network\n",
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, action_size, in_channels=3, cnn_outchannels=1, hidden_size=12):\n",
        "    super().__init__()\n",
        "  \n",
        "    self.cnn = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels=in_channels, out_channels=cnn_outchannels*16, kernel_size=3, stride=1, padding=1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Conv2d(in_channels=cnn_outchannels*16, out_channels=cnn_outchannels, kernel_size=3, stride=1, padding=1),\n",
        "                            nn.ReLU(),\n",
        "                            # nn.Conv2d(in_channels=cnn_outchannels*32, out_channels=cnn_outchannels, kernel_size=3, stride=1, padding=1),\n",
        "                            # nn.ReLU()\n",
        "                            )\n",
        "\n",
        "    for layer in self.cnn:\n",
        "      nn.init.uniform_(layer.weight) \n",
        "    # self.weight.data.uniform_(-1,1)                      \n",
        "\n",
        "    self.controller = nn.Sequential(\n",
        "                            nn.Linear(25*25, hidden_size),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Linear(hidden_size, action_size)\n",
        "                            )\n",
        "\n",
        "    for layer in self.controller:\n",
        "      nn.init.uniform_(layer.weight) \n",
        "\n",
        "    \n",
        "  def forward(self, img_batch):\n",
        "    \"\"\"Estimate q-values given image\n",
        "    \"\"\"\n",
        "    cnn_output = self.cnn(img_batch.permute([0, 3, 1, 2]))\n",
        "    return self.controller(cnn_output.view(cnn_output.size(0), -1)) # Have to resize the output of the cnn to be accepted  by the linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('frame', 'action', 'next_frame', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class TrainingStats():\n",
        "    def __init__(self, SAVE_FREQUENCY, SAVE_REWARD_THRESHOLD=5):\n",
        "        self.reward_results_dqn = []\n",
        "        self.episode_lengths = []\n",
        "        self.training_loss = []\n",
        "        self.global_steps = 0\n",
        "        self.episodes = 0\n",
        "        self.cum_reward = 0\n",
        "        self.curr_episode_len = 0\n",
        "        self.avg_reward = 0\n",
        "        self.prev_avg_reward = SAVE_REWARD_THRESHOLD\n",
        "        self.SAVE_REWARD_THRESHOLD = SAVE_REWARD_THRESHOLD\n",
        "        self.SAVE_FREQUENCY = SAVE_FREQUENCY\n",
        "\n",
        "    def add_loss(self, loss):\n",
        "        self.training_loss.append(loss)\n",
        "\n",
        "    def end_episode(self):\n",
        "        \"\"\"\n",
        "        Returns True if we want to save the network\n",
        "        \"\"\"\n",
        "        flag = False\n",
        "        self.reward_results_dqn.append(self.cum_reward)\n",
        "        self.episode_lengths.append(self.curr_episode_len)\n",
        "\n",
        "        # Save the network if save_frequency steps has passed and it is better than the previous avg_reward\n",
        "        if (self.global_steps % self.SAVE_FREQUENCY == 0) and (self.global_steps > self.SAVE_FREQUENCY):\n",
        "            avg_reward = sum(self.reward_results_dqn[-self.SAVE_FREQUENCY:]) / self.SAVE_FREQUENCY # get the average reward for the last SAVE_FREQUENCY steps\n",
        "            # print(f'{self.global_steps} steps. avg_reward: {avg_reward}')\n",
        "            if (avg_reward > self.prev_avg_reward + self.SAVE_REWARD_THRESHOLD):\n",
        "                print(f\"{self.global_steps} steps completed. Average Reward: {avg_reward} > Previous Average: {self.prev_avg_reward}. Saving Model\")\n",
        "                self.prev_avg_reward = avg_reward\n",
        "                flag = True\n",
        "\n",
        "        self.episodes += 1\n",
        "        self.cum_reward = 0  # Track cumulative reward per episode\n",
        "        self.curr_episode_len = 0\n",
        "\n",
        "        return flag\n",
        "\n",
        "    def end_frame(self, reward):\n",
        "        self.cum_reward += reward\n",
        "        self.global_steps += 1\n",
        "        self.curr_episode_len += 1\n",
        "\n",
        "    def see_results(self):\n",
        "        fig = plt.figure(figsize=(5,15))\n",
        "\n",
        "        ax1 = fig.add_subplot(311)\n",
        "        ax1.plot(self.episode_lengths)\n",
        "        plt.xlabel(\"Episode Number\")\n",
        "        plt.ylabel(\"Steps in Episode\")\n",
        "        plt.title(\"Episode Lengths\")\n",
        "        plt.grid(True)\n",
        "\n",
        "        ax2 = fig.add_subplot(312)\n",
        "        ax2.plot(self.reward_results_dqn)\n",
        "        plt.xlabel(\"Episode Number\")\n",
        "        plt.ylabel(\"Episode Reward\")\n",
        "        plt.title(\"Episode Rewards\")\n",
        "        plt.grid(True)\n",
        "\n",
        "        ax3 = fig.add_subplot(313)\n",
        "        ax3.plot(self.training_loss)\n",
        "        plt.xlabel(\"Episode Number\")\n",
        "        plt.ylabel(\"Loss Value\")\n",
        "        plt.title(\"Training Loss\")\n",
        "        plt.grid(True)\n",
        "\n",
        "        print(self.get_printout())\n",
        "\n",
        "    def get_printout(self):\n",
        "        return 'Global Steps: {:5d} Episodes: {:5d} Episode Length: {:4d} Reward: {:3.2f}'.format(self.global_steps, self.episodes, self.curr_episode_len, self.cum_reward)\n",
        "\n",
        "\n",
        "\n",
        "def validate_model(trained_model, action_space, pause_time=0.001, val_episodes=2, display=False):\n",
        "    episode_lengths = []\n",
        "    action_hist = []\n",
        "\n",
        "    for val_episode in range(1, val_episodes):\n",
        "        # Reset environment\n",
        "        sim, frame = reset_sim()\n",
        "        frame = torch.tensor(frame, dtype=torch.float32).unsqueeze(0).cuda()\n",
        "\n",
        "        done = False\n",
        "        episode_length = 0\n",
        "        angle = 0\n",
        "        prev_steer = 0\n",
        "        alpha = .25\n",
        "\n",
        "        # Begin episode\n",
        "        # while not done: \n",
        "        while True:\n",
        "            with torch.no_grad():\n",
        "                action_idx = trained_model(frame).max(1)[1].view(1, 1)\n",
        "\n",
        "            # Take step\n",
        "            observation, reward, done = sim.step(steer=action_space[action_idx], speed=1.0, display=display, validate=False)\n",
        "\n",
        "            next_frame = torch.tensor(observation, dtype=torch.float32).unsqueeze(0).cuda()\n",
        "\n",
        "            frame = next_frame  # Set current frame\n",
        "            episode_length += 1\n",
        "            time.sleep(pause_time)\n",
        "            \n",
        "        episode_lengths.append(episode_length)\n",
        "        episode_length = 0\n",
        "\n",
        "    # plt.plot(episode_lengths)\n",
        "    return action_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_mBUvXkT2dHy"
      },
      "outputs": [],
      "source": [
        "def get_action_dqn(network, frame, epsilon, epsilon_decay, action_size, global_steps, REPLAY_SIZE):\n",
        "  \"\"\"Select action according to e-greedy policy and decay epsilon\n",
        "  \"\"\"\n",
        "  if random.uniform(0., 1.) < max(0.1, epsilon) or global_steps < REPLAY_SIZE:\n",
        "    action = random.randint(0,action_size-1) #randint 0-action_space size corresponding to [-30,-20,-10,0,10,20,30] degrees\n",
        "    action = torch.tensor(action).view(1,1)\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      action = network(frame).max(1)[1].view(1, 1)\n",
        "  return action.cuda(), epsilon*epsilon_decay\n",
        "\n",
        "  \n",
        "def learn_dqn(memory_buffer, batch_size, optim, q_network, target_network, gamma, episode, target_update):\n",
        "  \"\"\"Update Q-Network according to DQN Loss function\n",
        "     Update Target Network every target_update global steps\n",
        "  \"\"\"\n",
        "  transitions = memory_buffer.sample(batch_size)\n",
        "  batch = Transition(*zip(*transitions))\n",
        "\n",
        "  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_frame)), dtype=torch.bool).cuda()\n",
        "  non_final_next_states = torch.cat([s for s in batch.next_frame\n",
        "                                          if s is not None])\n",
        "\n",
        "  frame_batch = torch.cat(batch.frame)\n",
        "  action_batch = torch.cat(batch.action).long()\n",
        "  reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "  Q = q_network(frame_batch).gather(1, action_batch)\n",
        "\n",
        "  Q_hat = torch.zeros(batch_size).cuda()\n",
        "  with torch.no_grad():\n",
        "      Q_hat[non_final_mask] = target_network(non_final_next_states).max(1)[0]\n",
        "\n",
        "  b = reward_batch + gamma*Q_hat\n",
        "  Q = torch.squeeze(Q)\n",
        "  \n",
        "  loss = F.mse_loss(Q, b)\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "\n",
        "  # torch.nn.utils.clip_grad_value_(q_network.parameters(), 100)\n",
        "  optim.step()\n",
        "\n",
        "  if episode % target_update == 0:\n",
        "    target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "  return loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCafVI552dgg"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sy_r9Wr2eg8",
        "outputId": "ed94a19d-7071-42cd-d62f-03014ea8158a"
      },
      "outputs": [],
      "source": [
        "def dqn_main(ts, episodes, action_space, lr, EPSILON_DECAY, START_TRAINING, MAX_EPISODE_LENGTH, LEARN_FREQUENCY, TARGET_UPDATE, BATCH_SIZE, SAVE_PATH, pretrained_model=None):\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Init networks\n",
        "  if pretrained_model == None:\n",
        "    q_network = QNetwork(action_size=len(action_space)).cuda()\n",
        "  else:\n",
        "    q_network = pretrained_model\n",
        "\n",
        "  target_network = QNetwork(action_size=len(action_space)).cuda()\n",
        "  target_network.load_state_dict(q_network.state_dict()) # copy q_network into target_network\n",
        "\n",
        "  optim = torch.optim.Adam(q_network.parameters(), lr=lr)  # Init optimizer\n",
        "  loop = tqdm(total=episodes, position=0, leave=False)\n",
        "  memory_buffer = ReplayMemory(45000)  # Init episode replay buffer\n",
        "\n",
        "  # HyperParameters that shouldn't change too much\n",
        "  epsilon = 1\n",
        "  CAR_SPEED = 1.0\n",
        "  gamma = 0.99\n",
        "  REPLAY_SIZE = 1000 # Don't let the network pick actions until after this many global steps\n",
        "\n",
        "  for episode in range(1, episodes):\n",
        "    sim, frame = reset_sim()     # Reset environment\n",
        "    frame = torch.tensor(frame, dtype=torch.float32).unsqueeze(0).cuda() * (1/255)\n",
        "    done = False\n",
        "    episode_losses = []\n",
        "\n",
        "    while not done and ts.curr_episode_len < MAX_EPISODE_LENGTH:\n",
        "      action_idx, epsilon = get_action_dqn(q_network, frame, epsilon, EPSILON_DECAY, len(action_space), ts.global_steps, REPLAY_SIZE) # Select e-greedy action\n",
        "\n",
        "      observation, reward, done = sim.step(steer=action_space[action_idx], speed=CAR_SPEED, display=False)       # Take step in the environment\n",
        "\n",
        "      next_frame = None if done else torch.tensor(observation, dtype=torch.float32).unsqueeze(0).cuda() * (1/255)\n",
        "      reward = torch.tensor([reward]).cuda()\n",
        "\n",
        "      # Store step in replay bufferimg\n",
        "      memory_buffer.push(frame, action_idx, next_frame, reward)\n",
        "      frame = next_frame                        # Set current frame\n",
        "      ts.end_frame(reward.item())\n",
        "\n",
        "      if (ts.episodes % LEARN_FREQUENCY) and (ts.global_steps > BATCH_SIZE):\n",
        "        loss = learn_dqn(memory_buffer, BATCH_SIZE, optim, q_network, target_network, gamma, episode, TARGET_UPDATE)     # Train\n",
        "        episode_losses.append(loss)\n",
        "    \n",
        "    if len(episode_losses) > 0:\n",
        "      ts.add_loss(sum(episode_losses)/len(episode_losses))\n",
        "    else:\n",
        "      ts.add_loss(np.nan)\n",
        "\n",
        "    loop.update(1)\n",
        "    a = torch.cuda.memory_allocated(0)\n",
        "    r = torch.cuda.memory_reserved(0)\n",
        "    loop.set_description('{} All Mem: {:.2f}, Res Mem: {:.2f}'.format(ts.get_printout(), a/1e9, r/1e9))\n",
        "\n",
        "    if ts.end_episode():\n",
        "      torch.save(q_network, f'{SAVE_PATH}{episode}.pt')\n",
        "  \n",
        "  return q_network, ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(model, SAVE_PATH):\n",
        "    torch.save(model, f'{SAVE_PATH}.pt')\n",
        "\n",
        "def load_model(action_size, PATH):\n",
        "    model = QNetwork(action_size)\n",
        "    model = torch.load(PATH)\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyper parameters\n",
        "########################################################################\n",
        "ts = TrainingStats(SAVE_FREQUENCY=5)\n",
        "episodes = 500\n",
        "action_space = [-30,30]                               # CHANGED\n",
        "lr = 0.00025\n",
        "EPSILON_DECAY = .999                                \n",
        "\n",
        "START_TRAINING = 200\n",
        "MAX_EPISODE_LENGTH = 6000 # A little over 3 minutes                                         \n",
        "LEARN_FREQUENCY = 3 \n",
        "                              \n",
        "TARGET_UPDATE = 250\n",
        "BATCH_SIZE = 32                                         \n",
        "SAVE_PATH = './rl_models/testing_'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'Conv2d' object cannot be interpreted as an integer",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#############################################################################\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m trained_model, ts \u001b[38;5;241m=\u001b[39m \u001b[43mdqn_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPSILON_DECAY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTART_TRAINING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_EPISODE_LENGTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARN_FREQUENCY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTARGET_UPDATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m save_model(trained_model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAVE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mfinalmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36mdqn_main\u001b[0;34m(ts, episodes, action_space, lr, EPSILON_DECAY, START_TRAINING, MAX_EPISODE_LENGTH, LEARN_FREQUENCY, TARGET_UPDATE, BATCH_SIZE, SAVE_PATH, pretrained_model)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Init networks\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained_model \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m   q_network \u001b[38;5;241m=\u001b[39m \u001b[43mQNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m   q_network \u001b[38;5;241m=\u001b[39m pretrained_model\n",
            "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mQNetwork.__init__\u001b[0;34m(self, action_size, in_channels, cnn_outchannels, hidden_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      7\u001b[0m                         nn\u001b[38;5;241m.\u001b[39mConv2d(in_channels\u001b[38;5;241m=\u001b[39min_channels, out_channels\u001b[38;5;241m=\u001b[39mcnn_outchannels\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m16\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      8\u001b[0m                         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[38;5;66;03m# nn.ReLU()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m                         )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn:\n\u001b[0;32m---> 16\u001b[0m   nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39muniform_(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mweight) \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# self.weight.data.uniform_(-1,1)                      \u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontroller \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     20\u001b[0m                         nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m25\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m25\u001b[39m, hidden_size),\n\u001b[1;32m     21\u001b[0m                         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m     22\u001b[0m                         nn\u001b[38;5;241m.\u001b[39mLinear(hidden_size, action_size)\n\u001b[1;32m     23\u001b[0m                         )\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:105\u001b[0m, in \u001b[0;36mSequential.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(OrderedDict(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems())[idx]))\n\u001b[1;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_item_by_idx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_modules\u001b[39m.\u001b[39;49mvalues(), idx)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:94\u001b[0m, in \u001b[0;36mSequential._get_item_by_idx\u001b[0;34m(self, iterator, idx)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get the idx-th item of the iterator\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m idx \u001b[39m=\u001b[39m operator\u001b[39m.\u001b[39;49mindex(idx)\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m-\u001b[39msize \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m idx \u001b[39m<\u001b[39m size:\n\u001b[1;32m     96\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mindex \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is out of range\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(idx))\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Conv2d' object cannot be interpreted as an integer"
          ]
        }
      ],
      "source": [
        "# Train Model\n",
        "#############################################################################\n",
        "start = time.time()\n",
        "trained_model, ts = dqn_main(ts, episodes, action_space, lr, EPSILON_DECAY, START_TRAINING, MAX_EPISODE_LENGTH, LEARN_FREQUENCY, TARGET_UPDATE, BATCH_SIZE, SAVE_PATH, \n",
        "                            pretrained_model=None)\n",
        "end = time.time()\n",
        "save_model(trained_model, f'{SAVE_PATH}finalmodel')\n",
        "ts.see_results()\n",
        "print(f\"Total Training Time: {(end-start)/60} min\")\n",
        "##############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x625 and 9216x128)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m action_space \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m30\u001b[39m]\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;28mlen\u001b[39m(action_space), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./rl_models/working_2actmodel.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpause_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[4], line 90\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(trained_model, action_space, pause_time, val_episodes, display)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 90\u001b[0m         action_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     93\u001b[0m         angle \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[0;34m(self, img_batch)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate q-values given image\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m cnn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(img_batch\u001b[38;5;241m.\u001b[39mpermute([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]))\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontroller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x625 and 9216x128)"
          ]
        }
      ],
      "source": [
        "# Validate Model\n",
        "# model = load_model(2, './rl_models/5actions_learnFreq_2169.pt')\n",
        "# model = load_model(2, './rl_models/working_model.pt')\n",
        "\n",
        "action_space = [-30, 30]\n",
        "model = load_model(len(action_space), './rl_models/3actions_base_finalmodel.pt')\n",
        "\n",
        "action_hist = validate_model(model, action_space, pause_time=0.001, display=True)\n",
        "\n",
        "plt.plot(action_hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([ 1.,  0.,  2.,  1.,  0.,  1.,  1.,  2.,  0.,  0.,  3.,  0.,  3.,\n",
              "         2.,  2.,  1.,  4.,  2.,  1.,  7.,  1.,  4.,  3.,  4.,  1.,  6.,\n",
              "         4.,  1.,  4.,  5.,  5.,  3.,  4.,  3.,  4.,  2.,  9.,  3.,  4.,\n",
              "         5.,  2., 10.,  5.,  6.,  5.,  3.,  3.,  6.,  5.,  3.,  1.,  6.,\n",
              "         5.,  3.,  5.,  3.,  4.,  4.,  8.,  2.,  5.,  3.,  3.,  4.,  3.,\n",
              "         5.,  3.,  5.,  2.,  2.,  2.,  2.,  4.,  0.,  5.,  3.,  1.,  1.,\n",
              "         2.,  1.,  4.,  3.,  2.,  2.,  0.,  2.,  2.,  3.,  0.,  2.,  2.,\n",
              "         1.,  0.,  2.,  2.,  0.,  2.,  4.,  4., 12.]),\n",
              " array([-24.94168048, -24.39255078, -23.84342108, -23.29429139,\n",
              "        -22.74516169, -22.19603199, -21.64690229, -21.09777259,\n",
              "        -20.54864289, -19.99951319, -19.45038349, -18.9012538 ,\n",
              "        -18.3521241 , -17.8029944 , -17.2538647 , -16.704735  ,\n",
              "        -16.1556053 , -15.6064756 , -15.0573459 , -14.50821621,\n",
              "        -13.95908651, -13.40995681, -12.86082711, -12.31169741,\n",
              "        -11.76256771, -11.21343801, -10.66430831, -10.11517861,\n",
              "         -9.56604892,  -9.01691922,  -8.46778952,  -7.91865982,\n",
              "         -7.36953012,  -6.82040042,  -6.27127072,  -5.72214102,\n",
              "         -5.17301133,  -4.62388163,  -4.07475193,  -3.52562223,\n",
              "         -2.97649253,  -2.42736283,  -1.87823313,  -1.32910343,\n",
              "         -0.77997374,  -0.23084404,   0.31828566,   0.86741536,\n",
              "          1.41654506,   1.96567476,   2.51480446,   3.06393416,\n",
              "          3.61306385,   4.16219355,   4.71132325,   5.26045295,\n",
              "          5.80958265,   6.35871235,   6.90784205,   7.45697175,\n",
              "          8.00610144,   8.55523114,   9.10436084,   9.65349054,\n",
              "         10.20262024,  10.75174994,  11.30087964,  11.85000934,\n",
              "         12.39913903,  12.94826873,  13.49739843,  14.04652813,\n",
              "         14.59565783,  15.14478753,  15.69391723,  16.24304693,\n",
              "         16.79217662,  17.34130632,  17.89043602,  18.43956572,\n",
              "         18.98869542,  19.53782512,  20.08695482,  20.63608452,\n",
              "         21.18521421,  21.73434391,  22.28347361,  22.83260331,\n",
              "         23.38173301,  23.93086271,  24.47999241,  25.02912211,\n",
              "         25.5782518 ,  26.1273815 ,  26.6765112 ,  27.2256409 ,\n",
              "         27.7747706 ,  28.3239003 ,  28.87303   ,  29.4221597 ,\n",
              "         29.97128939]),\n",
              " <BarContainer object of 100 artists>)"
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.hist(action_hist, bins=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Requirements:\n",
        "    - Drive Smoothly\n",
        "    - Drive on right hand side\n",
        "    - One place from us and one place from Dr. Lee (to start off)\n",
        "    - Not Crash for 3 minutes\n",
        "    - Random turns/no loops (less important)\n",
        "\n",
        "Next Steps:\n",
        "    - network architecture\n",
        "    - save better \n",
        "    - post processing the model output to smooth out the steering\n",
        "    \n",
        "    - normalization on the image input\n",
        "\n",
        "\n",
        "    - scaled rewards helped speed up the process\n",
        "\n",
        "    \n",
        "    - drive on right hand side\n",
        "\n",
        "    \n",
        "    - speeding up the tutorial code wouldn't hurt\n",
        "    - Get it to train on a more complicated (random?) map\n",
        "\n",
        "Next Next Steps:\n",
        "    - apply control input for 2+ frames?\n",
        "    - we are still not stacking images as input to the network\n",
        "\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
