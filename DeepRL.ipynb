{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl7hEuB3J4k3"
      },
      "source": [
        "## Objective\n",
        "\n",
        "- Build DQN and PPO Deep RL algorithms\n",
        "- Learn the difference between Q Learning and Policy Gradient techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVWokqnVab6O"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhaPOG6xt0yn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "Rim8iocC1Vva"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models, io\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "import glob\n",
        "import base64\n",
        "import cv2\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "from simulation import Simulator, coordinate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulator Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reset_sim():\n",
        "    \n",
        "    # Units are pixels for resolution, degrees for fov, degrees for angle, and pixels for height.\n",
        "    cameraSettings = {\n",
        "        \"resolution\": (1920, 1080),\n",
        "        \"fov\": {\"diagonal\": random.uniform(74, 80)}, # realsense diagonal fov is 77 degrees IIRC\n",
        "        \"angle\": {\"roll\": random.uniform(-5, 5), \"pitch\": random.uniform(10, 20), \"yaw\": random.uniform(-5, 5)}, # don't go too crazy with these, my code should be good up to like... 45 degrees probably? But the math gets unstable\n",
        "        \"height\": random.uniform(58, 74) # 8 pixels/inch - represents how high up the camera is relative to the road\n",
        "    }\n",
        "\n",
        "    mapParameters = {\n",
        "    \"loops\": 1,\n",
        "    \"size\": (6, 6),\n",
        "    \"expansions\": 5,\n",
        "    \"complications\": 4\n",
        "    }\n",
        "\n",
        "    # Can also pass car parameters for max/min speed, etc\n",
        "    carParameters = {\n",
        "        \"wheelbase\": random.uniform(5.5, 7.5), # inches, influences how quickly the steering will turn the car.  Larger = slower\n",
        "        \"maxSteering\": 30.0, # degrees, extreme (+ and -) values of steering\n",
        "        \"steeringOffset\": random.uniform(-.5, .5), # degrees, since the car is rarely perfectly aligned\n",
        "        \"minVelocity\": 0.0, # pixels/second, slower than this doesn't move at all.\n",
        "        \"maxVelocity\": 480.0, # pixels/second, 8 pixels/inch, so if the car can move 5 fps that gives us 480 pixels/s top speed\n",
        "    }\n",
        "\n",
        "    sim = Simulator(cameraSettings=cameraSettings)\n",
        "    \n",
        "    # startLocation = (random.randint(0, 5), random.randint(0, 5), 0, random.randint(0, 2))\n",
        "    # random seed for consistent maps\n",
        "    # can also pass a start location if you know the code: (y tile index, x tile index, position index, direction index)\n",
        "    # - position index is from 0-(number of connections the tile has - 1), so a straight is 0 or 1, a t is 0, 1, or 2.\n",
        "    # - direction index is 0 or 1 for normal or reversed.\n",
        "    sim.start(mapSeed='real', mapParameters=mapParameters, carParameters=carParameters, startPoint=(0,4,0,0))\n",
        "    where, facing = sim.RealSense.parent.ackermann.pose()\n",
        "    initial_img = sim.RealSense.camera.getImage(where, facing)\n",
        "    return sim, initial_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From here, the API for using the simulation is as follows. Steps the entire simulation, returns image, reward from sim.getReward() and a done bool (and we can change what 'done' means. Currently its if reward is negative):\n",
        "\n",
        "```python\n",
        "frame, reward, done = sim.step(steer, speed, display=False) \n",
        "```\n",
        "\n",
        "In order to reset the simulation, you just need to reconstruct the sim object and start it, using the reset_sim() function above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV282uYJ2aSw"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi_aDdTg2btp"
      },
      "source": [
        "Deep Q-Network (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) is a Q-learning algorithm that learns values for state-action pairs.\n",
        "\n",
        "Actions are sampled according to an $\\epsilon-greedy$ policy to help with exploration of the state space. Every time an action is sampled, the agent chooses a random action with $\\epsilon$ probability. Otherwise, the agent selects the action with the highest Q-value for a state. $\\epsilon$ decays over time according to $\\epsilon \\gets \\epsilon * epsilon\\_decay$.\n",
        "\n",
        "Tuples of state, action, reward, next_state, and terminal $(s,a,r,s',d)$ are collected during training. Every $learn\\_frequency$ steps $sample\\_size$ tuples are sampled and made into 5 tensors tensors of states, actions, rewarads, next_states, and terminals.\n",
        "\n",
        "The loss for a batch of size N is given below.\n",
        "\n",
        "$Loss=\\frac{1}{N}\\sum \\bigg(Q(s,a) - (r + \\gamma \\underset{a'\\sim A}{max} \\hat{Q}(s',a')(1-d))\\bigg)^2 $\n",
        "\n",
        "Loss is calculated and used to update the Q-Network. The target network $\\hat{Q}$ begins as a copy of the Q network but is not updated by the optimizer. Every $target\\_update$ steps, the target network is updated with the parameters of the Q-Network. This process is a type of bootstrapping.\n",
        "\n",
        "### TODO\n",
        "\n",
        "- Implement get action method with e-greedy policy\n",
        "- Implement sample batch method\n",
        "- Implement DQN learning algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "_mBUvXkT2dHy"
      },
      "outputs": [],
      "source": [
        "def get_action_dqn(network, state, epsilon, epsilon_decay):\n",
        "  \"\"\"Select action according to e-greedy policy and decay epsilon\n",
        "\n",
        "    Args:\n",
        "        network (QNetwork): Q-Network\n",
        "        state (np-array): current state, size (state_size)\n",
        "        epsilon (float): probability of choosing a random action\n",
        "        epsilon_decay (float): amount by which to decay epsilon\n",
        "\n",
        "    Returns:\n",
        "        action (int): chosen action [0, action_size)\n",
        "        epsilon (float): decayed epsilon\n",
        "  \"\"\"\n",
        "  \n",
        "  if random.uniform(0., 1.) < epsilon:\n",
        "    action = random.randint(0,6) #randint 0-6 corresponding to [-30,-20,-10,0,10,20,30] degrees\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      action = int(np.argmax(network(torch.Tensor(state).float().cuda()).cpu()))\n",
        "  return action, epsilon*epsilon_decay\n",
        "\n",
        "\n",
        "def prepare_batch(memory, trajectory_length):\n",
        "  \"\"\"Randomly sample batch from memory\n",
        "     Prepare cuda tensors\n",
        "\n",
        "    Args:\n",
        "        memory (list): state, action, next_state, reward, done tuples\n",
        "        batch_size (int): amount of memory to sample into a batch\n",
        "\n",
        "    Returns:\n",
        "        state (tensor): float cuda tensor of size (batch_size x state_size)\n",
        "        action (tensor): long tensor of size (batch_size)\n",
        "        next_state (tensor): float cuda tensor of size (batch_size x state_size)\n",
        "        reward (tensor): float cuda tensor of size (batch_size)\n",
        "        done (tensor): float cuda tensor of size (batch_size)\n",
        "  \"\"\"\n",
        "  frame = []\n",
        "  action = []\n",
        "  next_frame = []\n",
        "  reward = []\n",
        "  done = []\n",
        "\n",
        "  resize_transform = transforms.Resize(size=(72,128))\n",
        "  \n",
        "  # rand_mem = random.sample(memory, batch_size)\n",
        "  # frame = np.zeros((trajectory_length, *img_shape))\n",
        "  # action = np.zeros(trajectory_length)\n",
        "  # next_frame = np.zeros(frame.shape)\n",
        "  # reward = np.zeros(action.shape)\n",
        "  # m_done\n",
        "  \n",
        "  for m_frame, m_action, m_next_frame, m_reward, m_done in memory:\n",
        "    frame.append(cv2.resize(m_frame, (128,72)))\n",
        "    action.append(m_action)\n",
        "    next_frame.append(cv2.resize(m_next_frame, (128,72)))\n",
        "    reward.append(m_reward)\n",
        "    done.append(m_done)\n",
        "\n",
        "  frame = np.array(frame)\n",
        "  action = np.array(action)\n",
        "  next_frame = np.array(next_frame)\n",
        "  reward = np.array(reward)\n",
        "  done = np.array(done)\n",
        "\n",
        "  return torch.FloatTensor(frame).cuda(), torch.FloatTensor(action).cuda(),torch.FloatTensor(next_frame).cuda(), \\\n",
        "          torch.FloatTensor(reward).cuda(), torch.FloatTensor(done).cuda()\n",
        "  \n",
        "def learn_dqn(trajectory, optim, q_network, target_network, gamma, global_step, target_update):\n",
        "  \"\"\"Update Q-Network according to DQN Loss function\n",
        "     Update Target Network every target_update global steps\n",
        "\n",
        "    Args:\n",
        "        batch (tuple): tuple of state, action, next_state, reward, and done tensors\n",
        "        optim (Adam): Q-Network optimizer\n",
        "        q_network (QNetwork): Q-Network\n",
        "        target_network (QNetwork): Target Q-Network\n",
        "        gamma (float): discount factor\n",
        "        global_step (int): total steps taken in environment\n",
        "        target_update (int): frequency of target network update\n",
        "  \"\"\"\n",
        "  optim.zero_grad()\n",
        "  total_loss = torch.tensor([0.0], requires_grad=True).cuda()\n",
        "  episode_len = len(trajectory[0])\n",
        "\n",
        "\n",
        "  for time_step in range(episode_len):\n",
        "    state = trajectory[0][time_step] \n",
        "    action =trajectory[1][time_step] \n",
        "    next_state = trajectory[2][time_step] \n",
        "    reward = trajectory[3][time_step] \n",
        "    done = trajectory[4][time_step] \n",
        "\n",
        "    # Sequentially loop through the episode trajectory\n",
        "    action = torch.unsqueeze(action.unsqueeze(0),dim=0).long()\n",
        "\n",
        "    Q = torch.gather(q_network(state), 1, action)\n",
        "    Q_hat = torch.max(target_network(next_state), dim=1)[0]\n",
        "    b = (reward + gamma*Q_hat*(1 - done))\n",
        "\n",
        "    Q = torch.squeeze(Q)\n",
        "    b = torch.squeeze(b)\n",
        "    \n",
        "    total_loss = total_loss + F.mse_loss(Q, b)\n",
        "\n",
        "  total_loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  if global_step % target_update == 0:\n",
        "    target_network.load_state_dict(q_network.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGQgiY0WvImB"
      },
      "source": [
        "### Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "1vMhl-oevIBo"
      },
      "outputs": [],
      "source": [
        "# Q-Value Network\n",
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, feature_size = 1000, action_size = 7):\n",
        "    super().__init__()\n",
        "    \n",
        "    hidden_size = 100\n",
        "    \n",
        "    self.resnet50 = models.resnet18(models.ResNet18_Weights.DEFAULT)\n",
        "    self.resnet50.fc = nn.Linear(in_features=self.resnet50.fc.in_features, out_features=feature_size)\n",
        "\n",
        "    self.lstm = nn.LSTM(feature_size, hidden_size, num_layers=2, batch_first=True)\n",
        "    self.label = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    self.prev_hidden_state = torch.zeros(self.lstm.num_layers, hidden_size).cuda()\n",
        "    self.prev_cell_state =  torch.zeros(self.lstm.num_layers, hidden_size).cuda()\n",
        "    \n",
        "  def forward(self, img):\n",
        "    \"\"\"Estimate q-values given image\n",
        "\n",
        "      Args:\n",
        "          img batch (4d tensor): size (batch_size, channel, height, width)\n",
        "\n",
        "      Returns:\n",
        "          q-values (tensor): estimated q-values, size (batch x action_size)\n",
        "    \"\"\"\n",
        "    img = transforms.functional.convert_image_dtype(img).permute([2,0,1])\n",
        "    features = self.resnet50(img.unsqueeze(0))\n",
        "    output, (final_hidden_state, final_cell_state) = self.lstm(features, (self.prev_hidden_state, self.prev_cell_state))\n",
        "\n",
        "    self.prev_hidden_state = final_hidden_state.detach()\n",
        "    self.prev_cell_state = final_cell_state.detach()\n",
        "\n",
        "    # print(output.shape)\n",
        "    # print(final_hidden_state.shape)\n",
        "    # print(final_cell_state.shape)\n",
        "\n",
        "    values = self.label(output)\n",
        "    return values #size: (batch_size, action_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCafVI552dgg"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sy_r9Wr2eg8",
        "outputId": "ed94a19d-7071-42cd-d62f-03014ea8158a"
      },
      "outputs": [],
      "source": [
        "def dqn_main():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  # Hyper parameters\n",
        "  lr = 1e-3\n",
        "  episodes = 100\n",
        "  start_training = 1000\n",
        "  gamma = 0.99\n",
        "  batch_size = 32\n",
        "  epsilon = 1\n",
        "  epsilon_decay = .9999\n",
        "  target_update = 1000\n",
        "  learn_frequency = 2\n",
        "  save_frequency = 10  \n",
        "  MAX_EPISODE_LENGTH = 200\n",
        "\n",
        "  # Init networks\n",
        "  q_network = QNetwork().cuda()\n",
        "  target_network = QNetwork().cuda()\n",
        "  target_network.load_state_dict(q_network.state_dict()) #copy q_network into target_network\n",
        "\n",
        "  # Init optimizer\n",
        "  optim = torch.optim.Adam(q_network.parameters(), lr=lr)\n",
        "\n",
        "  # Init replay buffer\n",
        "  trajectory = []\n",
        "\n",
        "  # Begin main loop\n",
        "  results_dqn = []\n",
        "  loop = tqdm(total=episodes, position=0, leave=False)\n",
        "  action_space = [-30,-20,-10,0,10,20,30]\n",
        "\n",
        "  \n",
        "  for global_step, episode in enumerate(range(episodes)):\n",
        "\n",
        "    # Reset environment\n",
        "    sim, frame = reset_sim()\n",
        "    done = False\n",
        "    cum_reward = 0  # Track cumulative reward per episode\n",
        "\n",
        "    # Begin episode\n",
        "    while not done and cum_reward < MAX_EPISODE_LENGTH:  # End after 200 steps \n",
        "      # Select e-greedy action\n",
        "      action_idx, epsilon = get_action_dqn(q_network, frame, epsilon, epsilon_decay)\n",
        "\n",
        "      # Take step\n",
        "      next_frame, reward, done = sim.step(steer=action_space[action_idx], speed=1.5, display=True)\n",
        "\n",
        "      # Store step in replay buffer\n",
        "      trajectory.append((frame, action_idx, next_frame, reward, done))\n",
        "\n",
        "      cum_reward += reward\n",
        "      frame = next_frame  # Set current frame\n",
        "\n",
        "      # cv2.imshow(\"car\", next_frame)\n",
        "      # print(f\"Reward: {reward}, Action:{action_space[action_idx]}\")\n",
        "\n",
        "    # Train the network after episode ended\n",
        "    # Sample batch\n",
        "    batch = prepare_batch(trajectory, batch_size)\n",
        "    # Train\n",
        "    learn_dqn(batch, optim, q_network, target_network, gamma, global_step, target_update)\n",
        "      \n",
        "\n",
        "    if global_step % save_frequency == 0:\n",
        "      torch.save(q_network, f'./rl_models/model{global_step}.pt')\n",
        "\n",
        "    # Print results at end of episode\n",
        "    results_dqn.append(cum_reward)\n",
        "    loop.update(1)\n",
        "    loop.set_description('Episodes: {} Reward: {}'.format(episode, cum_reward))\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "  \n",
        "  return results_dqn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episodes: 9 Reward: 21.0:  10%|â–ˆ         | 10/100 [00:29<05:35,  3.73s/it] "
          ]
        }
      ],
      "source": [
        "results_dqn = dqn_main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[58.0]\n"
          ]
        }
      ],
      "source": [
        "print(results_dqn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ZWYwytCDC3aw",
        "outputId": "b3e3a1bb-55e6-4574-dad7-9daab7c8de4f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(results_dqn)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN9yy5EWVNz0"
      },
      "source": [
        "# PPO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvRUJUew0yN_"
      },
      "source": [
        "## Part 2\n",
        "\n",
        "Proximal Policy Optimization (https://arxiv.org/pdf/1707.06347.pdf) is a type of policy gradient method. Instead of calculating Q-values, we train a network $\\pi$ to optimize the probability of taking good actions directly, using states as inputs and actions as outputs. PPO also uses a value network $V$ that estimates state values in order to estimate the advantage $\\hat{A}$. \n",
        "\n",
        "Tuples of state, action distribution, action taken, and return $(s,\\pi(s), a,\\hat{R})$ are gathered for several rollouts. After training on this experience, these tuples are discarded and new experience is gathered.\n",
        "\n",
        "Loss for the value network and the policy network are calculated according to the following formula:\n",
        "\n",
        "$Loss=ValueLoss+PolicyLoss$\n",
        "\n",
        "$ValueLoss=\\frac{1}{N}\\sum \\bigg(\\hat{R} - V(s) \\bigg)^2 $\n",
        "\n",
        "$PolicyLoss=-\\frac{1}{N}\\sum \\min\\bigg( \\frac{\\pi'(a|s)}{\\pi(a|s)} \\hat{A}, clip(\\frac{\\pi'(a|s)}{\\pi(a|s)},1-\\epsilon,1+\\epsilon) \\hat{A} \\bigg) $\n",
        "\n",
        "$\\hat{R}_t = \\sum_{i=t}^H \\gamma^{i-1}r_i$\n",
        "\n",
        "$\\hat{A}_t=\\hat{R}_t-V(s_t)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsm1pILHVcEp"
      },
      "outputs": [],
      "source": [
        "def calculate_return(memory, rollout, gamma):\n",
        "  \"\"\"Return memory with calculated return in experience tuple\n",
        "\n",
        "    Args:\n",
        "        memory (list): (state, action, action_dist, return) tuples\n",
        "        rollout (list): (state, action, action_dist, reward) tuples from last rollout\n",
        "        gamma (float): discount factor\n",
        "\n",
        "    Returns:\n",
        "        list: memory updated with (state, action, action_dist, return) tuples from rollout\n",
        "  \"\"\"\n",
        "  return_val = 0\n",
        "  # rollout is reversed because we want to discount later return values\n",
        "  # Same as return_val = return_value*gamma**i (i is position in rollout) + curr_reward\n",
        "  for transition in reversed(rollout):\n",
        "    state, action, action_dist, reward = transition\n",
        "    return_val = reward + gamma*return_val\n",
        "    memory.append((state, action, action_dist, return_val))\n",
        "  return memory\n",
        "\n",
        "\n",
        "def get_action_ppo(network, state):\n",
        "  \"\"\"Sample action from the distribution obtained from the policy network\n",
        "\n",
        "    Args:\n",
        "        network (PolicyNetwork): Policy Network\n",
        "        state (np-array): current state, size (state_size)\n",
        "\n",
        "    Returns:\n",
        "        int: action sampled from output distribution of policy network\n",
        "        array: output distribution of policy network\n",
        "  \"\"\"\n",
        "  state = torch.unsqueeze(torch.Tensor(state).float().cuda(), dim=0)\n",
        "  with torch.no_grad(): # No grad offers a slight performance boost because it is not neccessary to do any sort of back prop when calling the network here\n",
        "    action_distribution = network(state)\n",
        "    action = int(torch.multinomial(action_distribution, 1)) # Samples one value from the action_distribution and returns the index of the action (in our case 0 or 1)\n",
        "  return action, action_distribution\n",
        "\n",
        "\n",
        "def learn_ppo(optim, policy, value, memory_dataloader, epsilon, policy_epochs):\n",
        "  \"\"\"Implement PPO policy and value network updates. Iterate over your entire \n",
        "     memory the number of times indicated by policy_epochs.    \n",
        "\n",
        "    Args:\n",
        "        optim (Adam): value and policy optimizer\n",
        "        policy (PolicyNetwork): Policy Network\n",
        "        value (ValueNetwork): Value Network\n",
        "        memory_dataloader (DataLoader): dataloader with (state, action, action_dist, return) tensors\n",
        "        epsilon (float): trust region\n",
        "        policy_epochs (int): number of times to iterate over all memory\n",
        "  \"\"\"\n",
        "  optim.zero_grad()\n",
        "  for epoch in range(policy_epochs):\n",
        "    for state, action, old_action_dist, returns in memory_dataloader:\n",
        "      state, old_action_dist, returns = state.float(), old_action_dist.float(), returns.float()\n",
        "\n",
        "      state = state.cuda()\n",
        "      value_state = torch.squeeze(value(state), dim=-1).cuda()\n",
        "      returns = returns.cuda()\n",
        "      action = action.cuda()\n",
        "\n",
        "      old_action_dist = torch.squeeze(old_action_dist, dim=1)\n",
        "      action = torch.unsqueeze(action, dim=-1)\n",
        "\n",
        "      value_loss = F.mse_loss(returns, value_state)\n",
        "      advantage = (returns - value_state).detach()\n",
        "\n",
        "      current_action_dist = policy(state)      \n",
        "\n",
        "      pr1 = torch.gather(current_action_dist, 1, action)\n",
        "      pr2 = torch.gather(old_action_dist, 1, action)\n",
        "      policy_ratio =  torch.squeeze(pr1 / pr2, dim=-1)\n",
        "\n",
        "      policy_gradient = policy_ratio * advantage\n",
        "\n",
        "      min = 1 - epsilon\n",
        "      max = 1 + epsilon\n",
        "      \n",
        "      clipped_policy_gradient = torch.clamp(policy_ratio, min, max) * advantage\n",
        "      # print(\"clipped_policy_gradient: \", clipped_policy_gradient.shape)\n",
        "      policy_loss = -torch.mean(torch.min(policy_gradient, clipped_policy_gradient))\n",
        "\n",
        "      loss = (value_loss + policy_loss)\n",
        "      loss.backward()\n",
        "\n",
        "      optim.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6RXma_-vSGX"
      },
      "source": [
        "### Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8URnP8xvTTG"
      },
      "outputs": [],
      "source": [
        "# Dataset that wraps memory for a dataloader\n",
        "class RLDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "    super().__init__()\n",
        "    self.data = []\n",
        "    for d in data:\n",
        "      self.data.append(d)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.data[index]\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "\n",
        "# Policy Network\n",
        "class PolicyNetwork(nn.Module):\n",
        "  def __init__(self, state_size, action_size):\n",
        "    super().__init__()\n",
        "    hidden_size = 8\n",
        "    \n",
        "    self.net = nn.Sequential(nn.Linear(state_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, action_size),\n",
        "                             nn.Softmax(dim=1))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\"Get policy from state\n",
        "\n",
        "      Args:\n",
        "          state (tensor): current state, size (batch x state_size)\n",
        "\n",
        "      Returns:\n",
        "          action_dist (tensor): probability distribution over actions (batch x action_size)\n",
        "    \"\"\"\n",
        "    return self.net(x)\n",
        "  \n",
        "\n",
        "# Value Network\n",
        "class ValueNetwork(nn.Module):\n",
        "  def __init__(self, state_size):\n",
        "    super().__init__()\n",
        "    hidden_size = 8\n",
        "  \n",
        "    self.net = nn.Sequential(nn.Linear(state_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, 1))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    \"\"\"Estimate value given state\n",
        "\n",
        "      Args:\n",
        "          state (tensor): current state, size (batch x state_size)\n",
        "\n",
        "      Returns:\n",
        "          value (tensor): estimated value, size (batch)\n",
        "    \"\"\"\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aBD_R_e01Qb"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX_Bv4M4MyY2",
        "outputId": "7886190c-fff5-443d-89ff-3c1d3fd7a71b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "def ppo_main():\n",
        "  # Hyper parameters\n",
        "  lr = 1e-3\n",
        "  epochs = 40\n",
        "  env_samples = 100\n",
        "  gamma = 0.9\n",
        "  batch_size = 256\n",
        "  epsilon = 0.2\n",
        "  policy_epochs = 5\n",
        "\n",
        "  # Init environment \n",
        "  state_size = 4\n",
        "  action_size = 2\n",
        "  env = gym.make('CartPole-v1')\n",
        "\n",
        "  # Init networks\n",
        "  policy_network = PolicyNetwork(state_size, action_size).cuda()\n",
        "  value_network = ValueNetwork(state_size).cuda()\n",
        "\n",
        "  # Init optimizer\n",
        "  optim = torch.optim.Adam(chain(policy_network.parameters(), value_network.parameters()), lr=lr)\n",
        "\n",
        "  # Start main loop\n",
        "  results_ppo = []\n",
        "  loop = tqdm(total=epochs, position=0, leave=False)\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    memory = []  # Reset memory every epoch\n",
        "    rewards = []  # Calculate average episodic reward per epoch\n",
        "\n",
        "    # Begin experience loop\n",
        "    for episode in range(env_samples):\n",
        "      \n",
        "      # Reset environment\n",
        "      state = env.reset()\n",
        "      done = False\n",
        "      rollout = []\n",
        "      cum_reward = 0  # Track cumulative reward\n",
        "\n",
        "      # Begin episode\n",
        "      while not done and cum_reward < 200:  # End after 200 steps   \n",
        "        # Get action\n",
        "        action, action_dist = get_action_ppo(policy_network, state)\n",
        "        \n",
        "        # Take step\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        # env.render()\n",
        "\n",
        "        # Store step\n",
        "        rollout.append((state, action, action_dist, reward))\n",
        "\n",
        "        cum_reward += reward\n",
        "        state = next_state  # Set current state\n",
        "\n",
        "      # Calculate returns and add episode to memory\n",
        "      memory = calculate_return(memory, rollout, gamma)\n",
        "\n",
        "      rewards.append(cum_reward)\n",
        "      ######################\n",
        "      env.close()\n",
        "      ######################\n",
        "    # Train\n",
        "    dataset = RLDataset(memory)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    learn_ppo(optim, policy_network, value_network, loader, epsilon, policy_epochs)\n",
        "    \n",
        "    # Print results\n",
        "    results_ppo.extend(rewards)  # Store rewards for this epoch\n",
        "    loop.update(1)\n",
        "    loop.set_description(\"Epochs: {} Reward: {}\".format(epoch, results_ppo[-1]))\n",
        "\n",
        "  return results_ppo\n",
        "\n",
        "results_ppo = ppo_main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "pLXetCMpC1DE",
        "outputId": "bfb657e4-24db-4a03-de19-3a65b11ca4ba"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(results_ppo)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
