{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhaPOG6xt0yn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rim8iocC1Vva"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "import gc\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "import time\n",
        "from simulation import Simulator, coordinate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulator Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reset_sim():\n",
        "    # Units are pixels for resolution, degrees for fov, degrees for angle, and pixels for height.\n",
        "    img_size = (128, 72)\n",
        "    cameraSettings = {\n",
        "        # \"resolution\": (1920, 1080),\n",
        "        \"resolution\": img_size,\n",
        "        \"fov\": {\"diagonal\": 77}, # realsense diagonal fov is 77 degrees IIRC\n",
        "        \"angle\": {\"roll\": 0, \"pitch\": 0, \"yaw\": 0}, # don't go too crazy with these, my code should be good up to like... 45 degrees probably? But the math gets unstable\n",
        "        # \"angle\": {\"roll\": 13, \"pitch\": 30, \"yaw\": 30}, # don't go too crazy with these, my code should be good up to like... 45 degrees probably? But the math gets unstable\n",
        "        \"height\": 66 # 8 pixels/inch - represents how high up the camera is relative to the road\n",
        "    }\n",
        "\n",
        "    mapParameters = {\n",
        "        \"loops\": 1,\n",
        "        \"size\": (6, 6),\n",
        "        \"expansions\": 5,\n",
        "        \"complications\": 4\n",
        "    }\n",
        "\n",
        "    # Can also pass car parameters for max/min speed, etc\n",
        "    carParameters = {\n",
        "        \"wheelbase\": 6.5, # inches, influences how quickly the steering will turn the car.  Larger = slower\n",
        "        \"maxSteering\": 30.0, # degrees, extreme (+ and -) values of steering\n",
        "        \"steeringOffset\": 0.0, # degrees, since the car is rarely perfectly aligned\n",
        "        \"minVelocity\": 0.0, # pixels/second, slower than this doesn't move at all.\n",
        "        \"maxVelocity\": 480.0, # pixels/second, 8 pixels/inch, so if the car can move 5 fps that gives us 480 pixels/s top speed\n",
        "    }\n",
        "\n",
        "    sim = Simulator(cameraSettings=cameraSettings)\n",
        "\n",
        "    startLocations = np.array([[0,1], [0,2], [0,3], [0,4], [0,5], [0,6], [0,7], [1,0], [1,1], [1,4], [1,7], [2,2], [2,3], [1,4], [2,5], [2,6], [2,0], [5,1], [3,2], [2,4], [5,5], [5,6], [2,7], [3,0], [7,1], [4,2], [7,3], [5,4], [6,5], [3,7], [4,0], [5,2], [7,4], [7,5], [4,7], [5,0], [7,2], [5,7], [6,0]])\n",
        "    startLoc = random.randint(0, 38)\n",
        "\n",
        "    sim.start(mapSeed='real', mapParameters=mapParameters, carParameters=carParameters, startPoint=(int(startLocations[startLoc, 0]),int(startLocations[startLoc, 1]),0,0))\n",
        "    # sim.start(mapSeed='real', mapParameters=mapParameters, carParameters=carParameters, startPoint=(0,4,0,0))\n",
        "    \n",
        "    where, facing = sim.RealSense.parent.ackermann.pose()\n",
        "    initial_img = sim.RealSense.camera.getImage(where, facing)\n",
        "    return sim, initial_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From here, the API for using the simulation is as follows. Steps the entire simulation, returns image, reward from sim.getReward() and a done bool (and we can change what 'done' means. Currently its if reward is negative):\n",
        "\n",
        "```python\n",
        "frame, reward, done = sim.step(steer, speed, display=False) \n",
        "```\n",
        "\n",
        "In order to reset the simulation, you just need to reconstruct the sim object and start it, using the reset_sim() function above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV282uYJ2aSw"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi_aDdTg2btp"
      },
      "source": [
        "Deep Q-Network (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) is a Q-learning algorithm that learns values for state-action pairs.\n",
        "\n",
        "Actions are sampled according to an $\\epsilon-greedy$ policy to help with exploration of the state space. Every time an action is sampled, the agent chooses a random action with $\\epsilon$ probability. Otherwise, the agent selects the action with the highest Q-value for a state. $\\epsilon$ decays over time according to $\\epsilon \\gets \\epsilon * epsilon\\_decay$.\n",
        "\n",
        "Tuples of state, action, reward, next_state, and terminal $(s,a,r,s',d)$ are collected during training. Every $learn\\_frequency$ steps $sample\\_size$ tuples are sampled and made into 5 tensors tensors of states, actions, rewarads, next_states, and terminals.\n",
        "\n",
        "The loss for a batch of size N is given below.\n",
        "\n",
        "$Loss=\\frac{1}{N}\\sum \\bigg(Q(s,a) - (r + \\gamma \\underset{a'\\sim A}{max} \\hat{Q}(s',a')(1-d))\\bigg)^2 $\n",
        "\n",
        "Loss is calculated and used to update the Q-Network. The target network $\\hat{Q}$ begins as a copy of the Q network but is not updated by the optimizer. Every $target\\_update$ steps, the target network is updated with the parameters of the Q-Network. This process is a type of bootstrapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q-Value Network\n",
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, action_size, in_channels=3, cnn_outchannels=1, hidden_size=128):\n",
        "    super().__init__()\n",
        "  \n",
        "    self.cnn = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels=in_channels, out_channels=cnn_outchannels*16, kernel_size=3, stride=1, padding=1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Conv2d(in_channels=cnn_outchannels*16, out_channels=cnn_outchannels*32, kernel_size=3, stride=1, padding=1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Conv2d(in_channels=cnn_outchannels*32, out_channels=cnn_outchannels, kernel_size=3, stride=1, padding=1),\n",
        "                            nn.ReLU()\n",
        "                            )\n",
        "\n",
        "    self.controller = nn.Sequential(\n",
        "                            nn.Linear(128*72, hidden_size),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Linear(hidden_size, action_size)\n",
        "                            )\n",
        "\n",
        "    \n",
        "  def forward(self, img_batch):\n",
        "    \"\"\"Estimate q-values given image\n",
        "    \"\"\"\n",
        "    cnn_output = self.cnn(img_batch.permute([0, 3, 1, 2]))\n",
        "    return self.controller(cnn_output.view(cnn_output.size(0), -1)) # Have to resize the output of the cnn to be accepted  by the linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('frame', 'action', 'next_frame', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class TrainingStats():\n",
        "    def __init__(self, SAVE_FREQUENCY, SAVE_REWARD_THRESHOLD=25):\n",
        "        self.reward_results_dqn = []\n",
        "        self.episode_lengths = []\n",
        "        self.global_steps = 0\n",
        "        self.episodes = 0\n",
        "        self.cum_reward = 0\n",
        "        self.curr_episode_len = 0\n",
        "        self.avg_reward = 0\n",
        "        self.prev_avg_reward = SAVE_REWARD_THRESHOLD\n",
        "        self.SAVE_REWARD_THRESHOLD = SAVE_REWARD_THRESHOLD\n",
        "        self.SAVE_FREQUENCY = SAVE_FREQUENCY\n",
        "\n",
        "    def end_episode(self):\n",
        "        \"\"\"\n",
        "        Returns True if we want to save the network\n",
        "        \"\"\"\n",
        "        flag = False\n",
        "        self.reward_results_dqn.append(self.cum_reward)\n",
        "        self.episode_lengths.append(self.curr_episode_len)\n",
        "\n",
        "        # Save the network if save_frequency steps has passed and it is better than the previous avg_reward\n",
        "        if (self.global_steps % self.SAVE_FREQUENCY == 0): # and self.global_steps > self.SAVE_FREQUENCY:\n",
        "            # avg_reward = sum(self.reward_results_dqn[:self.SAVE_FREQUENCY]) / self.SAVE_FREQUENCY # get the average reward for the last SAVE_FREQUENCY steps\n",
        "\n",
        "            # if (avg_reward > self.prev_avg_reward + self.SAVE_REWARD_THRESHOLD):\n",
        "            #     print(f\"{self.global_steps} steps completed. Average Reward: {avg_reward} > Previous Average: {self.prev_avg_reward}. Saving Model\")\n",
        "            # self.prev_avg_reward = avg_reward\n",
        "            flag = True\n",
        "            \n",
        "        self.episodes += 1\n",
        "        self.cum_reward = 0  # Track cumulative reward per episode\n",
        "        self.curr_episode_len = 0\n",
        "\n",
        "        return flag\n",
        "\n",
        "    def end_frame(self, reward):\n",
        "        self.cum_reward += reward\n",
        "        self.global_steps += 1\n",
        "        self.curr_episode_len += 1\n",
        "\n",
        "    def see_results(self):\n",
        "        # plt.subplot(121)\n",
        "        # plt.plot(self.reward_results_dqn[0::100])\n",
        "        # plt.xlabel(\"Episode Number\")\n",
        "        # plt.ylabel(\"Reward for the Episode\")\n",
        "        # plt.title(\"Training Rewards\")\n",
        "\n",
        "        plt.subplot(111)\n",
        "        plt.plot(self.episode_lengths[0::100])\n",
        "        plt.xlabel(\"Episode Number\")\n",
        "        plt.ylabel(\"Steps in Episode\")\n",
        "        plt.title(\"Episode Lengths\")\n",
        "\n",
        "        print(self.get_printout())\n",
        "\n",
        "\n",
        "    def get_printout(self):\n",
        "        return 'Global Steps: {:5d} Episodes: {:5d} Episode Length: {:4d} Reward: {:3.2f}'.format(self.global_steps, self.episodes, self.curr_episode_len, self.cum_reward)\n",
        "\n",
        "\n",
        "\n",
        "def validate_model(trained_model, action_space, pause_time=0.001, val_episodes=100):\n",
        "    episode_lengths = []\n",
        "\n",
        "    for val_episode in range(1, val_episodes):\n",
        "        # Reset environment\n",
        "        sim, frame = reset_sim()\n",
        "        frame = torch.tensor(frame, dtype=torch.float32).unsqueeze(0).cuda()\n",
        "\n",
        "        done = False\n",
        "        episode_length = 0\n",
        "\n",
        "        # Begin episode\n",
        "        while not done: \n",
        "            with torch.no_grad():\n",
        "                action_idx = trained_model(frame).max(1)[1].view(1, 1)\n",
        "\n",
        "            # Take step\n",
        "            observation, reward, done = sim.step(steer=action_space[action_idx], speed=1.5, display=True, validate=True)\n",
        "            next_frame = torch.tensor(observation, dtype=torch.float32).unsqueeze(0).cuda()\n",
        "\n",
        "            frame = next_frame  # Set current frame\n",
        "            episode_length += 1\n",
        "            time.sleep(pause_time)\n",
        "            \n",
        "        episode_lengths.append(episode_length)\n",
        "        episode_length = 0\n",
        "\n",
        "    plt.plot(episode_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_mBUvXkT2dHy"
      },
      "outputs": [],
      "source": [
        "def get_action_dqn(network, frame, epsilon, epsilon_decay, action_size, global_steps, REPLAY_SIZE):\n",
        "  \"\"\"Select action according to e-greedy policy and decay epsilon\n",
        "  \"\"\"\n",
        "  if random.uniform(0., 1.) < max(0.1, epsilon) or global_steps < REPLAY_SIZE:\n",
        "    action = random.randint(0,action_size-1) #randint 0-action_space size corresponding to [-30,-20,-10,0,10,20,30] degrees\n",
        "    action = torch.tensor(action).view(1,1)\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      action = network(frame).max(1)[1].view(1, 1)\n",
        "  return action.cuda(), epsilon*epsilon_decay\n",
        "\n",
        "  \n",
        "def learn_dqn(memory_buffer, batch_size, optim, q_network, target_network, gamma, episode, target_update):\n",
        "  \"\"\"Update Q-Network according to DQN Loss function\n",
        "     Update Target Network every target_update global steps\n",
        "  \"\"\"\n",
        "  transitions = memory_buffer.sample(batch_size)\n",
        "  batch = Transition(*zip(*transitions))\n",
        "\n",
        "  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_frame)), dtype=torch.bool).cuda()\n",
        "  non_final_next_states = torch.cat([s for s in batch.next_frame\n",
        "                                          if s is not None])\n",
        "\n",
        "  frame_batch = torch.cat(batch.frame)\n",
        "  action_batch = torch.cat(batch.action).long()\n",
        "  reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "  Q = q_network(frame_batch).gather(1, action_batch)\n",
        "\n",
        "  Q_hat = torch.zeros(batch_size).cuda()\n",
        "  with torch.no_grad():\n",
        "      Q_hat[non_final_mask] = target_network(non_final_next_states).max(1)[0]\n",
        "\n",
        "  b = reward_batch + gamma*Q_hat\n",
        "  Q = torch.squeeze(Q)\n",
        "  \n",
        "  loss = F.mse_loss(Q, b)\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "\n",
        "  torch.nn.utils.clip_grad_value_(q_network.parameters(), 100)\n",
        "  optim.step()\n",
        "\n",
        "  if episode % target_update == 0:\n",
        "    target_network.load_state_dict(q_network.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCafVI552dgg"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sy_r9Wr2eg8",
        "outputId": "ed94a19d-7071-42cd-d62f-03014ea8158a"
      },
      "outputs": [],
      "source": [
        "def dqn_main(ts, episodes, action_space, lr, EPSILON_DECAY, START_TRAINING, MAX_EPISODE_LENGTH, LEARN_FREQUENCY, TARGET_UPDATE, BATCH_SIZE, SAVE_PATH):\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Init networks\n",
        "  q_network = QNetwork(action_size=len(action_space)).cuda()\n",
        "  target_network = QNetwork(action_size=len(action_space)).cuda()\n",
        "  target_network.load_state_dict(q_network.state_dict()) # copy q_network into target_network\n",
        "\n",
        "  optim = torch.optim.Adam(q_network.parameters(), lr=lr)  # Init optimizer\n",
        "  loop = tqdm(total=episodes, position=0, leave=False)\n",
        "  memory_buffer = ReplayMemory(10000)  # Init episode replay buffer\n",
        "\n",
        "  # HyperParameters that shouldn't change too much\n",
        "  epsilon = 1\n",
        "  CAR_SPEED = 1.5\n",
        "  gamma = 0.99\n",
        "  REPLAY_SIZE = 1000 # Don't let the network pick actions until after this many global steps\n",
        "\n",
        "  for episode in range(1, episodes):\n",
        "    sim, frame = reset_sim()     # Reset environment\n",
        "    frame = torch.tensor(frame, dtype=torch.float32).unsqueeze(0).cuda()\n",
        "    done = False\n",
        "\n",
        "    while not done and ts.curr_episode_len < MAX_EPISODE_LENGTH:\n",
        "      action_idx, epsilon = get_action_dqn(q_network, frame, epsilon, EPSILON_DECAY, len(action_space), ts.global_steps, REPLAY_SIZE) # Select e-greedy action\n",
        "\n",
        "      observation, reward, done = sim.step(steer=action_space[action_idx], speed=CAR_SPEED, display=False)       # Take step in the environment\n",
        "\n",
        "      next_frame = None if done else torch.tensor(observation, dtype=torch.float32).unsqueeze(0).cuda()\n",
        "      reward = torch.tensor([reward]).cuda()\n",
        "\n",
        "      # Store step in replay bufferimg\n",
        "      memory_buffer.push(frame, action_idx, next_frame, reward)\n",
        "      frame = next_frame                        # Set current frame\n",
        "      ts.end_frame(reward.item())\n",
        "\n",
        "      if (ts.global_steps > START_TRAINING) and (ts.global_steps % LEARN_FREQUENCY == 0):\n",
        "        learn_dqn(memory_buffer, BATCH_SIZE, optim, q_network, target_network, gamma, episode, TARGET_UPDATE)     # Train\n",
        "\n",
        "    loop.update(1)\n",
        "    a = torch.cuda.memory_allocated(0)\n",
        "    r = torch.cuda.memory_reserved(0)\n",
        "    loop.set_description('{} All Mem: {:.2f}, Res Mem: {:.2f}'.format(ts.get_printout(), a/1e9, r/1e9))\n",
        "\n",
        "    if ts.end_episode():\n",
        "      torch.save(q_network, f'{SAVE_PATH}{episode}.pt')\n",
        "  \n",
        "  return q_network, ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(model, SAVE_PATH):\n",
        "    torch.save(model, f'{SAVE_PATH}.pt')\n",
        "\n",
        "def load_model(action_size, PATH):\n",
        "    model = QNetwork(action_size)\n",
        "    model = torch.load(PATH)\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global Steps: 135000 Episodes:  2168 Episode Length:  124 Reward: 123.00 All Mem: 1.14, Res Mem: 1.43:   5%|▌         | 2169/40000 [20:08<7:01:11,  1.50it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "135000 steps completed. Average Reward: 57.939 > Previous Average: 25. Saving Model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global Steps: 2928588 Episodes: 25406 Episode Length:  261 Reward: 260.00 All Mem: 1.14, Res Mem: 1.43:  64%|██████▎   | 25407/40000 [7:03:46<4:54:20,  1.21s/it]  "
          ]
        }
      ],
      "source": [
        "# Hyper parameters\n",
        "########################################################################\n",
        "ts = TrainingStats(SAVE_FREQUENCY=1000)\n",
        "episodes = 25000\n",
        "# action_space = [-30,-20,-10,0,10,20,30]             \n",
        "action_space = [-30,-15,0,15,30]                        \n",
        "lr = 0.00025\n",
        "EPSILON_DECAY = .9999                                \n",
        "\n",
        "START_TRAINING = 200\n",
        "MAX_EPISODE_LENGTH = 10000\n",
        "LEARN_FREQUENCY = 3                               \n",
        "TARGET_UPDATE = 2000\n",
        "BATCH_SIZE = 64                                         \n",
        "SAVE_PATH = './rl_models/5actions_learnFreq_'\n",
        "#############################################################################\n",
        "start = time.time()\n",
        "trained_model, ts = dqn_main(ts, episodes, action_space, lr, EPSILON_DECAY, START_TRAINING, MAX_EPISODE_LENGTH, LEARN_FREQUENCY, TARGET_UPDATE, BATCH_SIZE, SAVE_PATH)\n",
        "end = time.time()\n",
        "save_model(trained_model, f'{SAVE_PATH}final_model')\n",
        "ts.see_results()\n",
        "print(f\"Total Training Time: {(end-start)/60} min\")\n",
        "##############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model = load_model(2, './rl_models/5actions_learnFreq_2169.pt')\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model = load_model(2, './rl_models/working_model.pt')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./rl_models/working_2actmodel.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpause_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[4], line 98\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(trained_model, action_space, pause_time, val_episodes)\u001b[0m\n\u001b[1;32m     95\u001b[0m     action_idx \u001b[38;5;241m=\u001b[39m trained_model(frame)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Take step\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m observation, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m next_frame \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(observation, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    101\u001b[0m frame \u001b[38;5;241m=\u001b[39m next_frame  \u001b[38;5;66;03m# Set current frame\u001b[39;00m\n",
            "File \u001b[0;32m/auto/fsg/dcheney1/self-driving-cars/simulation/Simulator.py:103\u001b[0m, in \u001b[0;36mSimulator.step\u001b[0;34m(self, steer, speed, display, validate)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mArduino\u001b[39m.\u001b[39msetSpeed(speed)\n\u001b[1;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mArduino\u001b[39m.\u001b[39msetSteering(steer)\n\u001b[0;32m--> 103\u001b[0m frame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mRealSense\u001b[39m.\u001b[39;49mgetFrame()\n\u001b[1;32m    104\u001b[0m distToCenter, bearingOffset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetStats()\n\u001b[1;32m    106\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
            "File \u001b[0;32m/auto/fsg/dcheney1/self-driving-cars/simulation/Simulator.py:30\u001b[0m, in \u001b[0;36mRealSenseSimulator.getFrame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39madvanceTime(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[1;32m     29\u001b[0m where, facing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mackermann\u001b[39m.\u001b[39mpose()\n\u001b[0;32m---> 30\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcamera\u001b[39m.\u001b[39;49mgetImage(where, facing)\n",
            "File \u001b[0;32m/auto/fsg/dcheney1/self-driving-cars/simulation/camera.py:225\u001b[0m, in \u001b[0;36mCamera.getImage\u001b[0;34m(self, location, facing)\u001b[0m\n\u001b[1;32m    223\u001b[0m H \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA2\u001b[39m@\u001b[39m((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mR \u001b[39m@\u001b[39m extraR \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m extraT) \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA1)\n\u001b[1;32m    224\u001b[0m outImg \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mwarpPerspective(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbaseImage, H, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh), flags\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mINTER_CUBIC)\n\u001b[0;32m--> 225\u001b[0m points \u001b[39m=\u001b[39m calculatePolygon(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mh, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotXDeg, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotYDeg)\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m (points \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    227\u001b[0m     cv2\u001b[39m.\u001b[39mfillPoly(outImg, [points], (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m))\n",
            "File \u001b[0;32m/auto/fsg/dcheney1/self-driving-cars/simulation/polygonCalculator.py:177\u001b[0m, in \u001b[0;36mcalculatePolygon\u001b[0;34m(h, w, f, xAngle, yAngleInv)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m# print(\"calculating polygon with the following\", xAngle, yAngle, fov_y, yOffsetPixels, h, w, horizonCenterOffset, imageCenter)\u001b[39;00m\n\u001b[1;32m    176\u001b[0m horizonCenter \u001b[39m=\u001b[39m coordinate(horizonCenterOffset \u001b[39m+\u001b[39m imageCenter)\n\u001b[0;32m--> 177\u001b[0m horizonCenter\u001b[39m.\u001b[39;49mrotateAboutPoint(yAngle, imageCenter, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    178\u001b[0m leftDir \u001b[39m=\u001b[39m coordinate\u001b[39m.\u001b[39munitFromAngle(yAngle \u001b[39m+\u001b[39m \u001b[39m180\u001b[39m)\n\u001b[1;32m    179\u001b[0m rightDir \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mleftDir\n",
            "File \u001b[0;32m/auto/fsg/dcheney1/self-driving-cars/simulation/polygonCalculator.py:55\u001b[0m, in \u001b[0;36mcoordinate.rotateAboutPoint\u001b[0;34m(self, degrees, point, mutate)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m (point \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     53\u001b[0m     point \u001b[39m=\u001b[39m coordinate()\n\u001b[0;32m---> 55\u001b[0m newCoord \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39;49m \u001b[39m-\u001b[39;49m point)\u001b[39m.\u001b[39;49mrotate(degrees) \u001b[39m+\u001b[39m point\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m (mutate):\n\u001b[1;32m     57\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m newCoord\u001b[39m.\u001b[39mx\n",
            "File \u001b[0;32m/auto/fsg/dcheney1/self-driving-cars/simulation/polygonCalculator.py:44\u001b[0m, in \u001b[0;36mcoordinate.rotate\u001b[0;34m(self, degrees, mutate)\u001b[0m\n\u001b[1;32m     42\u001b[0m R \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(((c, \u001b[39m-\u001b[39ms, \u001b[39m0\u001b[39m), (s, c, \u001b[39m0\u001b[39m), (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)))\n\u001b[1;32m     43\u001b[0m homogenous \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my, \u001b[39m1\u001b[39m]])\u001b[39m.\u001b[39mT\n\u001b[0;32m---> 44\u001b[0m rotated \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49msqueeze((R\u001b[39m@homogenous\u001b[39;49m))\n\u001b[1;32m     45\u001b[0m newCoord \u001b[39m=\u001b[39m coordinate(\u001b[39mfloat\u001b[39m(rotated[\u001b[39m0\u001b[39m]), \u001b[39mfloat\u001b[39m(rotated[\u001b[39m1\u001b[39m]))\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m (mutate):\n",
            "File \u001b[0;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36msqueeze\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# model = load_model(2, './rl_models/5actions_learnFreq_2169.pt')\n",
        "# model = load_model(2, './rl_models/working_model.pt')\n",
        "model = load_model(2, './rl_models/working_2actmodel.pt')\n",
        "\n",
        "\n",
        "validate_model(model, [-30,30], pause_time=0.0, val_episodes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Next Steps:\n",
        "    - Get it to train on a more complicated (random?) map\n",
        "    - Dial in rewards?\n",
        "    - memory stuff with the replay buffer. Try to get things that are on the buffer not on CUDA. Everything on the buffer is on cuda. Before we had it where the buffer was not on cuda, but everything was put on CUDA when we were about to learn\n",
        "    - once we get a big enough buffer do we need to keep adding to it? is that increasingly helpful? This could speed up training if we don't always have to step through the environment\n",
        "    - we are still not stacking images as input to the network\n",
        "    - speeding up the tutorial code wouldn't hurt\n",
        "    - apply control input for 2+ frames?\n",
        "\"\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
