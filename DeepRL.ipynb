{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl7hEuB3J4k3"
      },
      "source": [
        "## Objective\n",
        "\n",
        "- Build DQN and PPO Deep RL algorithms\n",
        "- Learn the difference between Q Learning and Policy Gradient techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVWokqnVab6O"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhaPOG6xt0yn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rim8iocC1Vva"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/fsg/hps22/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models, io\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "import glob\n",
        "import base64\n",
        "import cv2\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "from simulation import Simulator, coordinate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulator Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reset_sim():\n",
        "    \n",
        "    # Units are pixels for resolution, degrees for fov, degrees for angle, and pixels for height.\n",
        "    cameraSettings = {\n",
        "        \"resolution\": (1920, 1080),\n",
        "        \"fov\": {\"diagonal\": random.uniform(74, 80)}, # realsense diagonal fov is 77 degrees IIRC\n",
        "        \"angle\": {\"roll\": random.uniform(-5, 5), \"pitch\": random.uniform(10, 20), \"yaw\": random.uniform(-5, 5)}, # don't go too crazy with these, my code should be good up to like... 45 degrees probably? But the math gets unstable\n",
        "        \"height\": random.uniform(58, 74) # 8 pixels/inch - represents how high up the camera is relative to the road\n",
        "    }\n",
        "\n",
        "    mapParameters = {\n",
        "    \"loops\": 1,\n",
        "    \"size\": (6, 6),\n",
        "    \"expansions\": 5,\n",
        "    \"complications\": 4\n",
        "    }\n",
        "\n",
        "    # Can also pass car parameters for max/min speed, etc\n",
        "    carParameters = {\n",
        "        \"wheelbase\": random.uniform(5.5, 7.5), # inches, influences how quickly the steering will turn the car.  Larger = slower\n",
        "        \"maxSteering\": 30.0, # degrees, extreme (+ and -) values of steering\n",
        "        \"steeringOffset\": random.uniform(-.5, .5), # degrees, since the car is rarely perfectly aligned\n",
        "        \"minVelocity\": 0.0, # pixels/second, slower than this doesn't move at all.\n",
        "        \"maxVelocity\": 480.0, # pixels/second, 8 pixels/inch, so if the car can move 5 fps that gives us 480 pixels/s top speed\n",
        "    }\n",
        "\n",
        "    sim = Simulator(cameraSettings=cameraSettings)\n",
        "    \n",
        "    # startLocation = (random.randint(0, 5), random.randint(0, 5), 0, random.randint(0, 2))\n",
        "    # random seed for consistent maps\n",
        "    # can also pass a start location if you know the code: (y tile index, x tile index, position index, direction index)\n",
        "    # - position index is from 0-(number of connections the tile has - 1), so a straight is 0 or 1, a t is 0, 1, or 2.\n",
        "    # - direction index is 0 or 1 for normal or reversed.\n",
        "    sim.start(mapSeed='real', mapParameters=mapParameters, carParameters=carParameters, startPoint=(0,4,0,0))\n",
        "    where, facing = sim.RealSense.parent.ackermann.pose()\n",
        "    initial_img = sim.RealSense.camera.getImage(where, facing)\n",
        "    return sim, initial_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From here, the API for using the simulation is as follows. Steps the entire simulation, returns image, reward from sim.getReward() and a done bool (and we can change what 'done' means. Currently its if reward is negative):\n",
        "\n",
        "```python\n",
        "frame, reward, done = sim.step(steer, speed, display=False) \n",
        "```\n",
        "\n",
        "In order to reset the simulation, you just need to reconstruct the sim object and start it, using the reset_sim() function above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV282uYJ2aSw"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi_aDdTg2btp"
      },
      "source": [
        "Deep Q-Network (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) is a Q-learning algorithm that learns values for state-action pairs.\n",
        "\n",
        "Actions are sampled according to an $\\epsilon-greedy$ policy to help with exploration of the state space. Every time an action is sampled, the agent chooses a random action with $\\epsilon$ probability. Otherwise, the agent selects the action with the highest Q-value for a state. $\\epsilon$ decays over time according to $\\epsilon \\gets \\epsilon * epsilon\\_decay$.\n",
        "\n",
        "Tuples of state, action, reward, next_state, and terminal $(s,a,r,s',d)$ are collected during training. Every $learn\\_frequency$ steps $sample\\_size$ tuples are sampled and made into 5 tensors tensors of states, actions, rewarads, next_states, and terminals.\n",
        "\n",
        "The loss for a batch of size N is given below.\n",
        "\n",
        "$Loss=\\frac{1}{N}\\sum \\bigg(Q(s,a) - (r + \\gamma \\underset{a'\\sim A}{max} \\hat{Q}(s',a')(1-d))\\bigg)^2 $\n",
        "\n",
        "Loss is calculated and used to update the Q-Network. The target network $\\hat{Q}$ begins as a copy of the Q network but is not updated by the optimizer. Every $target\\_update$ steps, the target network is updated with the parameters of the Q-Network. This process is a type of bootstrapping.\n",
        "\n",
        "### TODO\n",
        "\n",
        "- Implement get action method with e-greedy policy\n",
        "- Implement sample batch method\n",
        "- Implement DQN learning algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_mBUvXkT2dHy"
      },
      "outputs": [],
      "source": [
        "def get_action_dqn(network, state, epsilon, epsilon_decay):\n",
        "  \"\"\"Select action according to e-greedy policy and decay epsilon\n",
        "\n",
        "    Args:\n",
        "        network (QNetwork): Q-Network\n",
        "        state (np-array): current state, size (state_size)\n",
        "        epsilon (float): probability of choosing a random action\n",
        "        epsilon_decay (float): amount by which to decay epsilon\n",
        "\n",
        "    Returns:\n",
        "        action (int): chosen action [0, action_size)\n",
        "        epsilon (float): decayed epsilon\n",
        "  \"\"\"\n",
        "  \n",
        "  if random.uniform(0., 1.) < epsilon:\n",
        "    action = random.randint(0,6) #randint 0-6 corresponding to [-30,-20,-10,0,10,20,30] degrees\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      state_tensor = torch.Tensor(state).float().cuda()\n",
        "      # output = network(state_tensor).cpu()\n",
        "      # doutput = output.detach().cpu()\n",
        "      action = int(np.argmax(network(state_tensor).cpu()))\n",
        "      del state_tensor\n",
        "  return action, epsilon*epsilon_decay\n",
        "\n",
        "\n",
        "def prepare_batch(memory, trajectory_length):\n",
        "  \"\"\"Randomly sample batch from memory\n",
        "     Prepare cuda tensors\n",
        "\n",
        "    Args:\n",
        "        memory (list): state, action, next_state, reward, done tuples\n",
        "        batch_size (int): amount of memory to sample into a batch\n",
        "\n",
        "    Returns:\n",
        "        state (tensor): float cuda tensor of size (batch_size x state_size)\n",
        "        action (tensor): long tensor of size (batch_size)\n",
        "        next_state (tensor): float cuda tensor of size (batch_size x state_size)\n",
        "        reward (tensor): float cuda tensor of size (batch_size)\n",
        "        done (tensor): float cuda tensor of size (batch_size)\n",
        "  \"\"\"\n",
        "  frame = []\n",
        "  action = []\n",
        "  next_frame = []\n",
        "  reward = []\n",
        "  done = []\n",
        "\n",
        "  resize_transform = transforms.Resize(size=(72,128))\n",
        "  \n",
        "  # rand_mem = random.sample(memory, batch_size)\n",
        "  # frame = np.zeros((trajectory_length, *img_shape))\n",
        "  # action = np.zeros(trajectory_length)\n",
        "  # next_frame = np.zeros(frame.shape)\n",
        "  # reward = np.zeros(action.shape)\n",
        "  # m_done\n",
        "  \n",
        "  for m_frame, m_action, m_next_frame, m_reward, m_done in memory:\n",
        "    frame.append(cv2.resize(m_frame, (128,72)))\n",
        "    action.append(m_action)\n",
        "    next_frame.append(cv2.resize(m_next_frame, (128,72)))\n",
        "    reward.append(m_reward)\n",
        "    done.append(m_done)\n",
        "\n",
        "  frame = np.array(frame)\n",
        "  action = np.array(action)\n",
        "  next_frame = np.array(next_frame)\n",
        "  reward = np.array(reward)\n",
        "  done = np.array(done)\n",
        "\n",
        "  return torch.FloatTensor(frame).cuda(), torch.FloatTensor(action).cuda(),torch.FloatTensor(next_frame).cuda(), \\\n",
        "          torch.FloatTensor(reward).cuda(), torch.FloatTensor(done).cuda()\n",
        "  \n",
        "def learn_dqn(trajectory, optim, q_network, target_network, gamma, global_step, target_update):\n",
        "  \"\"\"Update Q-Network according to DQN Loss function\n",
        "     Update Target Network every target_update global steps\n",
        "\n",
        "    Args:\n",
        "        batch (tuple): tuple of state, action, next_state, reward, and done tensors\n",
        "        optim (Adam): Q-Network optimizer\n",
        "        q_network (QNetwork): Q-Network\n",
        "        target_network (QNetwork): Target Q-Network\n",
        "        gamma (float): discount factor\n",
        "        global_step (int): total steps taken in environment\n",
        "        target_update (int): frequency of target network update\n",
        "  \"\"\"\n",
        "  optim.zero_grad()\n",
        "  total_loss = torch.tensor([0.0], requires_grad=True).cuda()\n",
        "  episode_len = len(trajectory[0])\n",
        "\n",
        "\n",
        "  for time_step in range(episode_len):\n",
        "    state = trajectory[0][time_step] \n",
        "    action =trajectory[1][time_step] \n",
        "    next_state = trajectory[2][time_step] \n",
        "    reward = trajectory[3][time_step] \n",
        "    done = trajectory[4][time_step] \n",
        "\n",
        "    # Sequentially loop through the episode trajectory\n",
        "    action = torch.unsqueeze(action.unsqueeze(0),dim=0).long()\n",
        "\n",
        "    Q = torch.gather(q_network(state), 1, action)\n",
        "    Q_hat = torch.max(target_network(next_state), dim=1)[0]\n",
        "    b = (reward + gamma*Q_hat*(1 - done))\n",
        "\n",
        "    Q = torch.squeeze(Q)\n",
        "    b = torch.squeeze(b)\n",
        "    \n",
        "    total_loss = total_loss + F.mse_loss(Q, b)\n",
        "\n",
        "  total_loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  if global_step % target_update == 0:\n",
        "    target_network.load_state_dict(q_network.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGQgiY0WvImB"
      },
      "source": [
        "### Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1vMhl-oevIBo"
      },
      "outputs": [],
      "source": [
        "# Q-Value Network\n",
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, feature_size = 100, action_size = 7):\n",
        "    super().__init__()\n",
        "    \n",
        "    hidden_size = 10\n",
        "    \n",
        "    self.resnet50 = models.resnet18(models.ResNet18_Weights.DEFAULT)\n",
        "    self.resnet50.fc = nn.Linear(in_features=self.resnet50.fc.in_features, out_features=feature_size)\n",
        "\n",
        "    self.lstm = nn.LSTM(feature_size, hidden_size, num_layers=1, batch_first=True)\n",
        "    self.label = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    self.prev_hidden_state = torch.zeros(self.lstm.num_layers, hidden_size).cuda()\n",
        "    self.prev_cell_state =  torch.zeros(self.lstm.num_layers, hidden_size).cuda()\n",
        "    \n",
        "  def forward(self, img):\n",
        "    \"\"\"Estimate q-values given image\n",
        "\n",
        "      Args:\n",
        "          img batch (4d tensor): size (batch_size, channel, height, width)\n",
        "\n",
        "      Returns:\n",
        "          q-values (tensor): estimated q-values, size (batch x action_size)\n",
        "    \"\"\"\n",
        "    img = transforms.functional.convert_image_dtype(img).permute([2,0,1])\n",
        "    features = self.resnet50(img.unsqueeze(0))\n",
        "    output, (final_hidden_state, final_cell_state) = self.lstm(features, (self.prev_hidden_state, self.prev_cell_state))\n",
        "\n",
        "    self.prev_hidden_state = final_hidden_state.detach()\n",
        "    self.prev_cell_state = final_cell_state.detach()\n",
        "\n",
        "    # print(output.shape)\n",
        "    # print(final_hidden_state.shape)\n",
        "    # print(final_cell_state.shape)\n",
        "\n",
        "    values = self.label(output)\n",
        "    return values #size: (batch_size, action_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCafVI552dgg"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sy_r9Wr2eg8",
        "outputId": "ed94a19d-7071-42cd-d62f-03014ea8158a"
      },
      "outputs": [],
      "source": [
        "def dqn_main():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Hyper parameters\n",
        "  lr = 1e-3\n",
        "  episodes = 100\n",
        "  start_training = 1000\n",
        "  gamma = 0.99\n",
        "  batch_size = 32\n",
        "  epsilon = 1\n",
        "  epsilon_decay = .9999\n",
        "  target_update = 1000\n",
        "  learn_frequency = 2\n",
        "  save_frequency = 10  \n",
        "  MAX_EPISODE_LENGTH = 200\n",
        "\n",
        "  # Init networks\n",
        "  q_network = QNetwork().cuda()\n",
        "  target_network = QNetwork().cuda()\n",
        "  target_network.load_state_dict(q_network.state_dict()) #copy q_network into target_network\n",
        "\n",
        "  # Init optimizer\n",
        "  optim = torch.optim.Adam(q_network.parameters(), lr=lr)\n",
        "\n",
        "  # Init replay buffer\n",
        "  trajectory = []\n",
        "\n",
        "  # Begin main loop\n",
        "  results_dqn = []\n",
        "  loop = tqdm(total=episodes, position=0, leave=False)\n",
        "  action_space = [-30,-20,-10,0,10,20,30]\n",
        "\n",
        "  \n",
        "  for global_step, episode in enumerate(range(episodes)):\n",
        "\n",
        "    # Reset environment\n",
        "    sim, frame = reset_sim()\n",
        "    done = False\n",
        "    cum_reward = 0  # Track cumulative reward per episode\n",
        "\n",
        "    # Begin episode\n",
        "    while not done and cum_reward < MAX_EPISODE_LENGTH:  # End after 200 steps \n",
        "      # Select e-greedy action\n",
        "      action_idx, epsilon = get_action_dqn(q_network, frame, epsilon, epsilon_decay)\n",
        "\n",
        "      # Take step\n",
        "      next_frame, reward, done = sim.step(steer=action_space[action_idx], speed=1.5, display=False)\n",
        "\n",
        "      # Store step in replay buffer\n",
        "      trajectory.append((frame, action_idx, next_frame, reward, done))\n",
        "\n",
        "      cum_reward += reward\n",
        "      frame = next_frame  # Set current frame\n",
        "\n",
        "      # cv2.imshow(\"car\", next_frame)\n",
        "      # print(f\"Reward: {reward}, Action:{action_space[action_idx]}\")\n",
        "\n",
        "    # Train the network after episode ended\n",
        "    # Sample batch\n",
        "    batch = prepare_batch(trajectory, batch_size)\n",
        "    # Train\n",
        "    learn_dqn(batch, optim, q_network, target_network, gamma, global_step, target_update)\n",
        "    del batch\n",
        "      \n",
        "\n",
        "    if global_step % save_frequency == 0:\n",
        "      torch.save(q_network, f'./rl_models/model{global_step}.pt')\n",
        "\n",
        "    t = torch.cuda.get_device_properties(0).total_memory\n",
        "    r = torch.cuda.memory_reserved(0)\n",
        "    a = torch.cuda.memory_allocated(0)\n",
        "    f = r-a  # free inside reserved\n",
        "\n",
        "    # Print results at end of episode\n",
        "    results_dqn.append(cum_reward)\n",
        "    loop.update(1)\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    loop.set_description('Episodes: {} Reward: {} Allocated Memory: {} Reserved Memory {}'.format(episode, cum_reward, a / 1e9, r / 1e9))\n",
        "  \n",
        "  return results_dqn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.\n",
            "  warnings.warn(\n",
            "Episodes: 6 Reward: 49.0 Allocated Memory: 0.276000768 Reserved Memory 2.518679552:   7%|â–‹         | 7/100 [06:45<50:45, 32.75s/it]   "
          ]
        }
      ],
      "source": [
        "results_dqn = dqn_main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8360230912 0 0\n"
          ]
        }
      ],
      "source": [
        "# q_network = QNetwork()\n",
        "# total = 0\n",
        "# for parameter in q_network.parameters():\n",
        "#     total += np.prod(parameter.size())\n",
        "# print(total)\n",
        "\n",
        "t = torch.cuda.get_device_properties(0).total_memory\n",
        "r = torch.cuda.memory_reserved(0)\n",
        "a = torch.cuda.memory_allocated(0)\n",
        "\n",
        "print(t, r, a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[58.0]\n"
          ]
        }
      ],
      "source": [
        "print(results_dqn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ZWYwytCDC3aw",
        "outputId": "b3e3a1bb-55e6-4574-dad7-9daab7c8de4f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(results_dqn)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
